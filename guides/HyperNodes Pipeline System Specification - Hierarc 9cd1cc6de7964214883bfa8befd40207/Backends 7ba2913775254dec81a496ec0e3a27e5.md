**Backends** determine both *how* (execution strategy) and *where* (infrastructure) nodes are executed.

**Backend = Infrastructure + Execution Strategy**

The system uses a **hybrid approach**: each Backend has smart defaults but accepts an optional Engine parameter for advanced use cases.

## Execution Modes

Backends control execution along **two independent dimensions**:

### 1. Node Execution

How nodes execute **within a single** [`pipeline.run](http://pipeline.run)()` call.

**`node_execution="sequential"`** (default for LocalBackend)

- Nodes execute one at a time, in topological order
- Simple for loop: `for node in execution_order: execute(node)`
- Predictable, easy to debug
- Best for: Development, debugging, CPU-bound work

**`node_execution="async"`**

- Independent nodes execute concurrently using `asyncio`
- Multiple nodes can make I/O calls simultaneously
- Still single-process, no true parallelism
- Best for: I/O-bound pipelines (API calls, file I/O, database queries)

**`node_execution="parallel"`**

- Independent nodes execute in true parallel (threads/processes/workers)
- Multiple nodes can use multiple CPU cores simultaneously
- Best for: CPU-bound work, GPU workloads, cloud backends

**Example:**

```python
# Sequential: A→B→C→D (even if B and C are independent)
backend = LocalBackend(node_execution="sequential")

# Async: A→(B,C)→D (B and C run concurrently if independent)
backend = LocalBackend(node_execution="async")

# Parallel: A→(B,C)→D (B and C run in parallel processes)
backend = LocalBackend(node_execution="parallel", max_workers=4)
```

---

### 2. Map Execution

How items are processed **within** [`pipeline.map](http://pipeline.map)(items)` calls.

**`map_execution="sequential"`** (default for LocalBackend)

- Items processed one at a time in a simple loop
- `for item in items: [pipeline.run](http://pipeline.run)(item)`
- No concurrency across items
- Best for: Development, debugging, understanding execution flow

**`map_execution="async"`**

- Multiple items processed concurrently using `asyncio`
- Items don't block each other during I/O
- Still single-process
- Best for: I/O-bound per-item work (downloading, API calls per item)

**`map_execution="parallel"`**

- Multiple items processed in true parallel
- Distributes items across processes/workers
- Best for: CPU-bound per-item work, large batches, cloud backends

**Example:**

```python
# Sequential map: process items one by one
backend = LocalBackend(
    node_execution="async",      # Nodes within each item run async
    map_execution="sequential"   # But items processed one at a time
)

# Parallel map: process multiple items simultaneously
backend = LocalBackend(
    node_execution="sequential",  # Nodes within each item run sequentially
    map_execution="parallel"      # But multiple items processed in parallel
)

# Both parallel: maximum concurrency
backend = LocalBackend(
    node_execution="parallel",   # Nodes within each item run in parallel
    map_execution="parallel",    # AND multiple items processed in parallel
    max_workers=8
)
```

---

### Independence of Node and Map Execution

These two dimensions are **completely independent**. You can mix and match:

| Configuration | Behavior |
| --- | --- |
| `node_execution="sequential"`, `map_execution="sequential"` | Simplest: one node at a time, one item at a time |
| `node_execution="async"`, `map_execution="sequential"` | Nodes concurrent within each item, but items processed serially |
| `node_execution="sequential"`, `map_execution="parallel"` | Each item processed simply, but many items at once |
| `node_execution="async"`, `map_execution="parallel"` | Maximum concurrency: nodes concurrent + items parallel |

**Common patterns:**

```python
# Pattern 1: I/O-bound pipeline, small batch
# Async nodes (for I/O), sequential map (to avoid overwhelming API rate limits)
backend = LocalBackend(node_execution="async", map_execution="sequential")

# Pattern 2: CPU-bound nodes, large batch
# Sequential nodes (simple), parallel map (distribute items)
backend = LocalBackend(node_execution="sequential", map_execution="parallel")

# Pattern 3: GPU workload on cloud
# Parallel nodes (use GPU), parallel map (distribute across many GPUs)
backend = ModalBackend(
    gpu="A100",
    node_execution="parallel",
    map_execution="parallel",
    max_concurrent=50
)
```

---

## Selective Computation (Framework-Level)

**You can compute only specific outputs** instead of running the entire pipeline. The framework automatically determines and executes only the required upstream dependencies.

This is a **framework feature**, not a backend setting—it works the same way regardless of which backend you use.

### Specifying Outputs in [pipeline.run](http://pipeline.run)()

**Default behavior (no outputs specified):**

Returns all node outputs as a dictionary.

```python
# Runs all nodes, returns all outputs
result = [pipeline.run](http://pipeline.run)(input_data)  
# result = {"node1": ..., "node2": ..., "node3": ...}
```

**Selective computation:**

Specify which outputs you need via `output_nodes` or `output_name`:

```python
# Only compute node_5 and its upstream dependencies
# (If pipeline has 100 nodes, only runs ~5 needed nodes)
result = [pipeline.run](http://pipeline.run)(input_data, output_nodes=["node_5"])
# result = {"node_5": ...}

# Single output as direct value (not dict)
result = [pipeline.run](http://pipeline.run)(input_data, output_name="node_5")
# result = <direct value from node_5>
```

**When to use:**

- Large pipelines where you only need specific outputs
- Debugging specific nodes without running the full pipeline
- Avoiding expensive computations in unused branches

**Works with `.map()` too:**

```python
# Only compute final_summary for each item
results = [pipeline.map](http://pipeline.map)(items, output_name="final_summary")
```

### Batch-Aware Execution (Future)

Optimized scheduling for batch workloads (e.g., when using [`pipeline.map`](http://pipeline.map)`()`).

**Potential optimizations:**

- Group operations across items before proceeding to next stage
- Balance between parallelism and batching efficiency
- Dynamic scheduling based on resource availability

---

# LocalBackend (Phase 1 - Current)

**Simple sequential execution (for loop over nodes)**

```python
from pipeline_system import LocalBackend

# Default backend (sequential execution)
backend = LocalBackend()

# Or simply omit backend parameter - LocalBackend is the default
pipeline = Pipeline(nodes=[...])

result = [pipeline.run](http://pipeline.run)(...) 
```

**Phase 1 Implementation:**

- Sequential execution (simple for loop)
- Single-process Python
- No parallelism yet
- Good for: Initial development, debugging, small data

**Future phases will add:**

- `LocalBackend(engine="threaded")` - Multi-threaded for I/O-bound tasks
- `LocalBackend(engine="multiprocess")` - Multi-process for CPU-bound tasks
- `LocalBackend(engine=DaftEngine(...))` - Daft dataframe engine for batch processing

**Future phases will add:**

- `LocalBackend(engine="threaded")` - Multi-threaded for I/O-bound tasks
- `LocalBackend(engine="multiprocess")` - Multi-process for CPU-bound tasks

- `LocalBackend(engine=DaftEngine(...))` - Daft dataframe engine for batch processing

---

## Implementation Pseudocode

### LocalBackend

```python
class LocalBackend:
    def __init__(
        self,
        node_execution: Literal["sequential", "async", "parallel"] = "sequential",
        map_execution: Literal["sequential", "async", "parallel"] = "sequential",
        max_workers: Optional[int] = None
    ):
        self.node_execution = node_execution
        [self.map](http://self.map)_execution = map_execution
        self.max_workers = max_workers or os.cpu_count()
    
    def run(self, pipeline: Pipeline, inputs: dict) -> dict:
        """Execute a single [pipeline.run](http://pipeline.run)()"""
        # Dispatch based on node_execution mode
        if self.node_execution == "sequential":
            return self._run_sequential(pipeline, inputs)
        elif self.node_execution == "async":
            return [asyncio.run](http://asyncio.run)(self._run_async(pipeline, inputs))
        elif self.node_execution == "parallel":
            return self._run_parallel(pipeline, inputs)
    
    def _run_sequential(self, pipeline, inputs):
        """Simple loop - one node at a time"""
        results = {**inputs}
        execution_order = pipeline.get_execution_order()
        
        for node in execution_order:
            # Check if this is a nested pipeline
            if isinstance(node, Pipeline):
                # IMPORTANT: Nested pipeline uses ITS OWN backend!
                # It might be completely different (e.g., ModalBackend)
                nested_inputs = {k: results[k] for k in node.input_names}
                nested_results = [node.backend.run](http://node.backend.run)(node, nested_inputs)
                results.update(nested_results)
            else:
                # Regular function node
                node_inputs = {k: results[k] for k in node.input_names}
                results[node.output_name] = node.func(**node_inputs)
        
        return results
    
    async def _run_async(self, pipeline, inputs):
        """Concurrent execution using asyncio - for I/O bound work"""
        results = {**inputs}
        execution_order = pipeline.get_execution_order()
        pending_nodes = set(execution_order)
        running_tasks = {}
        
        while pending_nodes or running_tasks:
            # Find nodes ready to execute (all deps satisfied)
            ready_nodes = [
                n for n in pending_nodes 
                if all(dep in results for dep in n.dependencies)
            ]
            
            # Start tasks for ready nodes
            for node in ready_nodes:
                if isinstance(node, Pipeline):
                    # Nested pipeline - dispatch to ITS backend
                    nested_inputs = {k: results[k] for k in node.input_names}
                    # The nested pipeline might use a completely different backend!
                    task = asyncio.create_task(
                        self._run_nested_async(node, nested_inputs)
                    )
                else:
                    # Regular node
                    node_inputs = {k: results[k] for k in node.input_names}
                    task = asyncio.create_task(
                        self._execute_node_async(node, node_inputs)
                    )
                running_tasks[task] = node
                pending_nodes.remove(node)
            
            # Wait for at least one task to complete
            if running_tasks:
                done, _ = await asyncio.wait(
                    running_tasks.keys(), 
                    return_when=asyncio.FIRST_COMPLETED
                )
                
                for task in done:
                    node = running_tasks.pop(task)
                    result = await task
                    results[node.output_name] = result
        
        return results
    
    def map(self, pipeline: Pipeline, items: List, inputs: dict) -> List[dict]:
        """Execute [pipeline.map](http://pipeline.map)() over multiple items"""
        
        if [self.map](http://self.map)_execution == "sequential":
            # Simple loop - one item at a time
            return [
                [self.run](http://self.run)(pipeline, {**inputs, **item})
                for item in items
            ]
        
        elif [self.map](http://self.map)_execution == "async":
            # Concurrent I/O for multiple items
            return [asyncio.run](http://asyncio.run)(self._map_async(pipeline, items, inputs))
        
        elif [self.map](http://self.map)_execution == "parallel":
            # True parallelism across processes
            with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [
                    executor.submit([self.run](http://self.run), pipeline, {**inputs, **item})
                    for item in items
                ]
                return [f.result() for f in futures]
    
    async def _map_async(self, pipeline, items, inputs):
        """Run multiple items concurrently using asyncio"""
        tasks = [
            asyncio.create_task(
                # Each item still goes through [self.run](http://self.run)()
                # which respects node_execution setting!
                self._run_async_wrapper(pipeline, {**inputs, **item})
            )
            for item in items
        ]
        return await asyncio.gather(*tasks)
```

### ModalBackend

```python
class ModalBackend:
    """Remote execution on Modal - parallel by default"""
    
    def __init__(
        self,
        gpu: str,
        memory: str,
        node_execution: Literal["async", "parallel"] = "parallel",
        map_execution: Literal["async", "parallel"] = "parallel",
        max_concurrent: int = 100
    ):
        self.gpu = gpu
        self.memory = memory
        self.node_execution = node_execution
        [self.map](http://self.map)_execution = map_execution
        self.max_concurrent = max_concurrent
        # Note: No "sequential" option - this is a cloud backend!
    
    def run(self, pipeline: Pipeline, inputs: dict) -> dict:
        """Dispatch single run to Modal"""
        # Serialize and submit to Modal
        payload = serialize(pipeline, inputs)
        result = self._remote_execute.remote(payload)
        return result
    
    @modal.function(gpu="{self.gpu}", memory="{self.memory}")
    def _remote_execute(self, payload):
        """Executes on Modal infrastructure"""
        pipeline, inputs = deserialize(payload)
        
        # Execute with specified node_execution strategy
        if self.node_execution == "async":
            return [asyncio.run](http://asyncio.run)(self._run_async(pipeline, inputs))
        else:  # parallel
            return self._run_parallel(pipeline, inputs)
    
    def map(self, pipeline: Pipeline, items: List, inputs: dict) -> List[dict]:
        """Execute map over Modal infrastructure"""
        
        if [self.map](http://self.map)_execution == "parallel":
            # Modal's native .map() - distributes across workers
            payloads = [
                serialize(pipeline, {**inputs, **item}) 
                for item in items
            ]
            results = self._remote_[execute.map](http://execute.map)(
                payloads,
                kwargs={"max_concurrent": self.max_concurrent}
            )
            return list(results)
        
        elif [self.map](http://self.map)_execution == "async":
            # Submit all items, Modal handles async scheduling
            # Still executes remotely, just with async coordination
            return self._map_async_on_modal(pipeline, items, inputs)
```

### Key Implementation Details

**Nested Pipeline Handling**

When a backend encounters a nested pipeline during execution, it **delegates to that pipeline's own backend**:

```python
# In any backend's execution loop:
if isinstance(node, Pipeline):
    # Don't use [self.run](http://self.run)()! Use the nested pipeline's backend!
    nested_results = [node.backend.run](http://node.backend.run)(node, nested_inputs)
```

This enables powerful patterns like:

```python
# Outer: Local sequential
outer = Pipeline(
    nodes=[load_data, gpu_pipeline, save_results],
    backend=LocalBackend(node_execution="sequential")
)

# Inner: Remote parallel on GPU
gpu_pipeline = Pipeline(
    nodes=[encode, transform],
    backend=ModalBackend(
        gpu="A100",
        node_execution="parallel"  # Different from outer!
    )
)

# Execution:
# 1. load_data runs locally (sequential)
# 2. gpu_pipeline dispatches to Modal (runs in parallel on GPU)
# 3. save_results runs locally (sequential) with results from Modal
```

**Map + Nested Pipelines**

When using `.map()` with nested pipelines, both dimensions apply:

```python
# Example: Process many items, each on GPU
outer = Pipeline(
    nodes=[preprocess, gpu_pipeline, postprocess],
    backend=LocalBackend(
        node_execution="sequential",
        map_execution="parallel"  # Process multiple items in parallel
    )
)

gpu_pipeline = Pipeline(
    nodes=[encode, transform],
    backend=ModalBackend(
        node_execution="parallel"  # Nodes within each item run in parallel on GPU
    )
)

# When calling [outer.map](http://outer.map)(many_items):
# - LocalBackend processes multiple items in parallel (map_execution="parallel")
# - For each item, when it hits gpu_pipeline:
#   - Dispatches to Modal
#   - Modal runs encode + transform in parallel (node_execution="parallel")
#   - Returns results to LocalBackend
# - Results from all items collected in parallel
```

---

# Remote Backends

## ModalBackend

**Serverless GPU execution on Modal Labs**

ModalBackend executes your pipeline on [Modal](https://modal.com) serverless infrastructure. Each pipeline execution runs in an isolated container with your specified resources (GPU, memory, CPU).

### Philosophy: Simple & Just Works

Phase 1 focuses on **simplicity**:

- **One pipeline per container**: Each [`pipeline.run](http://pipeline.run)()` gets a fresh Modal container
- **Basic batching for `.map()`**: Items distributed across multiple Modal containers
- **No complex orchestration**: Direct serialization and execution

### Basic Configuration

```python
from hypernodes import Pipeline, ModalBackend
import modal

# Minimal configuration
backend = ModalBackend(
    image=modal.Image.debian_slim(python_version="3.12"),
    gpu="A100",
    memory="32GB"
)

pipeline = Pipeline(
    nodes=[encode, transform, aggregate],
    backend=backend
)
```

### Environment Syncing

**Option 1: Sync Local uv Project (Recommended)**

```python
import modal

# Auto-sync your local uv environment
image = (
    modal.Image.debian_slim(python_version="3.12")
    .uv_sync()  # Reads pyproject.toml + uv.lock
    .add_local_python_source("hypernodes")  # Your pipeline code
)

backend = ModalBackend(image=image, gpu="A100")
```

**Option 2: Manual Package Installation**

```python
image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install("torch==2.3.0", "transformers>=4.40.0", "numpy", "pandas")
    .add_local_python_source("hypernodes")
)

backend = ModalBackend(image=image, gpu="A100")
```

### Working with Data and Models

**Small Files (Code, Configs)**: Use `.add_local_dir()`

```python
image = (
    modal.Image.debian_slim()
    .uv_sync()
    .add_local_python_source("hypernodes")
    .add_local_dir(
        local_path="./configs",
        remote_path="/configs",
        copy=False  # Added at startup, not baked into image
    )
)
```

**Large Files (Data, Models)**: Use Modal Volumes

```python
import modal

# Create volumes (once)
data_volume = modal.Volume.from_name("my-data", create_if_missing=True)
models_volume = modal.Volume.from_name("my-models", create_if_missing=True)

# Configure backend with volumes
backend = ModalBackend(
    image=image,
    volumes={
        "/data": data_volume,      # Large data files
        "/models": models_volume,  # Model weights
    },
    gpu="A100"
)
```

### Volume Operations

**Upload Data (Local → Modal)**

```python
# Python API
import modal

vol = modal.Volume.from_name("my-data")

with vol.batch_upload() as batch:
    batch.put_file("./local_data.parquet", "/data.parquet")
    batch.put_directory("./datasets/", "/datasets")
```

```bash
# CLI
modal volume put my-data ./local_data.parquet /data.parquet
modal volume ls my-data /
```

**Download Results (Modal → Local)**

```python
# Python API
vol = modal.Volume.from_name("my-data")

data = b""
for chunk in [vol.read](http://vol.read)_file("/results.json"):
    data += chunk

with open("./results.json", "wb") as f:
    f.write(data)
```

```bash
# CLI
modal volume get my-data /results.json ./results.json
```

### Complete Example

```python
"""
Local structure:
├── pyproject.toml
├── uv.lock
├── hypernodes/
│   └── [pipeline.py](http://pipeline.py)
├── configs/
│   └── model_config.yaml
├── data/          # Large files → Volume
└── models/        # Large files → Volume
"""

import modal
from hypernodes import Pipeline, ModalBackend, node

# Step 1: Create volumes for large data
data_volume = modal.Volume.from_name("data", create_if_missing=True)
models_volume = modal.Volume.from_name("models", create_if_missing=True)

# Step 2: Upload data once
with data_volume.batch_upload() as batch:
    batch.put_directory("./data/", "/")

with models_volume.batch_upload() as batch:
    batch.put_directory("./models/", "/")

# Step 3: Build image
image = (
    modal.Image.debian_slim(python_version="3.12")
    .uv_sync()
    .add_local_python_source("hypernodes")
    .add_local_dir("./configs", "/configs")
)

# Step 4: Configure backend
backend = ModalBackend(
    image=image,
    volumes={
        "/data": data_volume,
        "/models": models_volume,
    },
    gpu="A100",
    memory="32GB",
    timeout=3600
)

# Step 5: Use in pipeline
@node(output_name="embeddings")
def encode(data_path: str):
    import pandas as pd
    import torch
    
    # Read from volume
    df = [pd.read](http://pd.read)_parquet(data_path)  # /data/passages.parquet
    model = torch.load("/models/[encoder.pt](http://encoder.pt)")
    
    return model.encode(df["text"].tolist())

pipeline = Pipeline(nodes=[encode], backend=backend)

# Paths on Modal:
# - Code: /root/hypernodes/...
# - Configs: /configs/model_config.yaml
# - Data: /data/passages.parquet
# - Models: /models/[encoder.pt](http://encoder.pt)

result = [pipeline.run](http://pipeline.run)(data_path="/data/passages.parquet")
```

### File Organization Reference

| **Content Type** | **Method** | **Why** |
| --- | --- | --- |
| Python packages | `.uv_sync()` or `.pip_install()` | Managed dependencies |
| Your code | `.add_local_python_source("hypernodes")` | Fast iteration |
| Small configs | `.add_local_dir("./configs", "/configs")` | Quick access |
| Large data files | Upload to Volume, mount with `volumes={}` | Persistent, shared |
| Model weights | Upload to Volume, mount at `/models` | Persistent, large |
| Temporary files | Container disk | Ephemeral |

### Configuration Parameters

```python
backend = ModalBackend(
    image: modal.Image,              # Required: Modal image with dependencies
    gpu: Optional[str] = None,       # GPU type: "A100", "A10G", "T4", "any", None
    memory: Optional[str] = None,    # Memory: "32GB", "256GB", etc.
    cpu: Optional[float] = None,     # CPU cores: 1.0, 2.0, 8.0, etc.
    timeout: int = 3600,             # Max execution time in seconds
    volumes: dict = {},              # Volume mounts: {"/path": modal.Volume}
    secrets: list = [],              # Modal secrets for API keys, etc.
    max_concurrent: int = 100,       # Max parallel containers for .map()
)
```

### Execution Behavior

**For [`pipeline.run](http://pipeline.run)()`**:

- Creates a single Modal container
- Serializes pipeline and inputs
- Executes remotely
- Returns results

**For [`pipeline.map](http://pipeline.map)(items)`**:

- Creates multiple Modal containers (up to `max_concurrent`)
- Distributes items across containers
- Executes in parallel
- Returns list of results

### Implementation Status

**Phase 1 (Current):**

- ✅ Basic remote execution
- ✅ Image configuration with uv/pip
- ✅ Volume mounting for data/models
- ✅ GPU/memory/timeout configuration
- ✅ Basic batching for `.map()`

**Future Enhancements:**

- Advanced orchestration (multiple pipelines per container)
- Streaming results
- Auto-scaling configuration
- Cost optimization hints

## CoiledBackend

**Cloud VMs with Dask**

```python
# Phase N: Remote execution on Coiled
backend = CoiledBackend(
    memory="512 GB",
    region="us-east-1"
    # Uses Dask execution by default
)
```

---

# Architecture Rationale

**Why Hybrid Backend/Engine?**

1. **Simple by default**: `LocalBackend()` just works with sensible defaults
2. **Composable when needed**: Advanced users can override execution strategy
3. **Clear separation**: Backend = infrastructure, Engine = execution strategy
4. **Realistic**: Acknowledges that some combinations make sense (Daft on Modal) while others don't (Dask on Modal)

**Implementation Note**: Each Backend validates if the engine is supported:

- LocalBackend supports: sequential (default), threaded, multiprocess, Daft
- ModalBackend supports: built-in (default), Daft, maybe Ray
- CoiledBackend supports: Dask (default), maybe Daft

---

# Nested Pipelines and Backends

**Each pipeline can define its own backend**, including nested pipelines. This enables:

- Hybrid execution workflows (local + remote)
- Optimized resource allocation per pipeline stage
- Independent backend configuration at each nesting level

## Example: Hierarchical Backend Configuration

```python
# GPU-intensive inner pipeline
gpu_pipeline = Pipeline(
    nodes=[encode_images, compute_embeddings],
    backend=ModalBackend(
        gpu="A100",
        memory="256 GB",
        engine=DaftEngine(partitions=50)
    )
)

# CPU-bound middle pipeline  
cpu_pipeline = Pipeline(
    nodes=[preprocess, gpu_pipeline, postprocess],
    backend=CoiledBackend(
        memory="128 GB",
        workers=20
    )
)

# Local orchestration
main_pipeline = Pipeline(
    nodes=[load_data, cpu_pipeline, save_results],
    backend=LocalBackend()
)

# Execution flow:
# 1. load_data: Local
# 2. cpu_pipeline dispatched to Coiled:
#    a. preprocess: Coiled workers
#    b. gpu_pipeline dispatched to Modal:
#       - encode_images: Modal A100 GPU
#       - compute_embeddings: Modal A100 GPU
#    c. postprocess: Coiled workers (receives results from Modal)
# 3. save_results: Local (receives results from Coiled)
result = main_[pipeline.run](http://pipeline.run)(data=dataset)
```

## Context Propagation Across Backends

When nested pipelines use different backends, context propagation ensures:

- **Telemetry continuity**: Traces link across local→remote→local boundaries
- **Cache coherence**: Cache lookups work across backend transitions
- **Error context**: Failures in nested pipelines include full execution path

See [Tracing & Telemetry](https://www.notion.so/Tracing-Telemetry-da0bddf3d656448e99f2b968fd8c2b49?pvs=21) for details on context propagation.

## Configuration Inheritance

Backends are part of the hierarchical configuration inheritance system. See the **Hierarchical Configuration Precedence** section in [Core Concepts](https://www.notion.so/Core-Concepts-4a4dd7402980462eb83fc2b3d5059ccc?pvs=21) for complete details on how configuration cascades through nested pipelines.

**Backend inheritance behavior:**

By default, nested pipelines inherit the parent's backend if none is specified:

```python
# Outer pipeline uses ModalBackend
outer = Pipeline(
    nodes=[node_a, inner_pipeline, node_b],
    backend=ModalBackend(gpu="A100")
)

# Inner pipeline inherits ModalBackend(gpu="A100") if no backend specified
inner_pipeline = Pipeline(
    nodes=[node_x, node_y]
    # No backend specified → inherits from parent
)
```

**To override inheritance, explicitly set the backend:**

```python
inner_pipeline = Pipeline(
    nodes=[node_x, node_y],
    backend=LocalBackend()  # Explicit override
)
```

**Inheritance is recursive** through multiple nesting levels:

```python
# Level 1: Local execution
level_1 = Pipeline(
    nodes=[...],
    backend=LocalBackend()
)

# Level 2: Override to Modal GPU
level_2 = Pipeline(
    nodes=[...],
    backend=ModalBackend(gpu="A100")  # Override
)

# Level 3: No backend specified
level_3 = Pipeline(
    nodes=[...]
    # Inherits ModalBackend from level_2 (most recent override)
)
```

Backend inheritance works seamlessly with other configuration options (cache, callbacks, telemetry), allowing selective optimization at each nesting level
# Daft Code Generation - Complete & Fixed

## Summary

The Daft code generation feature is now **fully functional** and generates complete, executable Daft code that exactly mirrors the internal Daft operations performed by `DaftEngine`.

## What Was Fixed

### Problem 1: Incomplete Code Generation
**Issue**: Generated code was stopping prematurely during map operations, only showing the explode operation and missing:
- DataFrame creation (`daft.from_pydict`)
- Inner pipeline operations
- Groupby aggregation
- Output selection and collection

**Root Cause**: In code generation mode, when processing PipelineNodes (map operations), the code tried to access DataFrame columns that didn't exist yet (because no actual execution was happening), causing a `ValueError` that was silently caught.

**Fix**: Modified `_convert_mapped_pipeline_node()` to skip DataFrame introspection when in code generation mode and instead track column availability through node metadata.

### Problem 2: Missing DataFrame Creation
**Issue**: Generated code started with `df = df.with_column(...)` without first creating `df`.

**Root Cause**: DataFrame creation was only added at the end of conversion, after determining which inputs were stateful.

**Fix**: Already implemented correctly - DataFrame creation is inserted at the beginning of the generated code after conversion completes.

## How It Works

### Code Generation Flow

1. **User calls `pipeline.show_daft_code(inputs={...})`**
2. **Creates temporary pipeline** with `DaftEngine(code_generation_mode=True)`
3. **Runs pipeline** (doesn't execute, just generates code):
   - Converts each node to UDF definition + operation code
   - Tracks stateful inputs (objects that need pre-initialization)
   - Builds list of imports and operations
4. **Assembles complete code**:
   - Imports
   - UDF definitions
   - DataFrame creation (with actual input data)
   - All pipeline operations
   - Output selection and collection
5. **Returns executable Python string**

### Generated Code Structure

```python
"""
Generated Daft code - Exact translation from HyperNodes pipeline.
...
"""

import daft

# ==================== Stateful Objects ====================
# (Comments showing what needs to be initialized)

# ==================== UDF Definitions ====================
# All @daft.func and @daft.cls wrappers

# ==================== Pipeline Execution ====================

# Create DataFrame with input data
df = daft.from_pydict({...})

# Add stateful objects
df = df.with_column("encoder", daft.lit(encoder))

# All pipeline operations
df = df.with_column(...)
df = df.explode(...)
df = df.groupby(...).agg(...)

# Select output columns
df = df.select(...)

# Collect results
result = df.collect()
print(result.to_pydict())
```

## Usage Examples

### Basic Usage

```python
from hypernodes import Pipeline, node

@node(output_name="result")
def process(x: int) -> int:
    return x * 2

pipeline = Pipeline(nodes=[process])

# Generate code
code = pipeline.show_daft_code(inputs={"x": 5})
print(code)

# Save to file
with open("generated.py", "w") as f:
    f.write(code)
```

### With Map Operations

```python
# Single-item pipeline
single = Pipeline(nodes=[process_item], name="single")

# Mapped version
mapped = single.as_node(
    input_mapping={"items": "item"},
    output_mapping={"processed": "results"},
    map_over="items"
)

full_pipeline = Pipeline(nodes=[load_data, mapped])

# Shows how map translates to explode + groupby
code = full_pipeline.show_daft_code(inputs={"path": "data.csv"})
```

### With Stateful Objects

```python
# Create stateful object BEFORE passing to pipeline
encoder = ColBERTEncoder(model_name="...", trust_remote_code=True)

inputs = {
    "corpus_path": "data/corpus.parquet",
    "encoder": encoder,  # Pass pre-initialized object
    ...
}

# Generated code will show where encoder is used
code = pipeline.show_daft_code(inputs=inputs)
```

## Analyzing Your Hebrew Retrieval Pipeline

### Generate the Code

```python
# In your retrieval script
code = pipeline.show_daft_code(
    inputs=inputs,
    output_name="evaluation_results"
)

# Save for analysis
with open("retrieval_daft_code.py", "w") as f:
    f.write(code)
```

### What to Look For

1. **Map Operation Cycles**
   ```python
   # Each .map() becomes:
   df = df.explode(daft.col("items"))
   # ... inner operations ...
   df = df.groupby(...).agg(...)
   ```
   - Count how many cycles you have
   - Each cycle materializes data

2. **Stateful Object Passing**
   ```python
   # Is encoder being passed through every row?
   df = df.with_column("encoded", encode(df["text"], df["encoder"]))
   ```
   - Check if using @daft.cls properly
   - Verify initialization happens once

3. **Unnecessary Columns**
   - Are we keeping columns we don't need?
   - Can we drop intermediate results earlier?

4. **UDF Definitions**
   - Are they using @daft.func or @daft.cls appropriately?
   - Can any be replaced with Daft-native operations?

### Comparing Performance

```python
# Time the HyperNodes version
import time

start = time.time()
results_hypernodes = pipeline.run(inputs=inputs, output_name="evaluation_results")
time_hypernodes = time.time() - start

# Hand-optimize the generated code
# ... write optimized version ...

start = time.time()
results_optimized = run_optimized_daft()
time_optimized = time.time() - start

print(f"HyperNodes:  {time_hypernodes:.2f}s")
print(f"Optimized:   {time_optimized:.2f}s")
print(f"Speedup:     {time_hypernodes/time_optimized:.2f}x")
```

## Technical Details

### Key Implementation Changes

**File**: `src/hypernodes/integrations/daft/engine.py`

1. **Added code generation mode flag**:
   ```python
   def __init__(self, ..., code_generation_mode: bool = False):
       self.code_generation_mode = code_generation_mode
       if code_generation_mode:
           self._init_code_generation()
   ```

2. **Skip DataFrame introspection in code gen mode**:
   ```python
   if not self.code_generation_mode:
       # Actually access DataFrame columns
       rename_exprs.append(df[outer_name].alias(inner_name))
   else:
       # Just track what should be available
       inner_available.add(inner_name)
   ```

3. **Generate code instead of executing**:
   ```python
   if self.code_generation_mode:
       self._generate_operation_code(f'df = df.with_column(...)')
       return df  # Return unchanged df
   else:
       return df.with_column(...)  # Actually execute
   ```

**File**: `src/hypernodes/pipeline.py`

4. **Added public method**:
   ```python
   def show_daft_code(self, inputs: Dict[str, Any], 
                      output_name: Union[str, List[str], None] = None) -> str:
       code_engine = DaftEngine(code_generation_mode=True)
       temp_pipeline = self.with_engine(code_engine)
       temp_pipeline.run(inputs=inputs, output_name=output_name)
       return code_engine.get_generated_code()
   ```

### Testing

All tests pass (25 tests total):
- 13 code generation tests
- 12 Daft backend tests

```bash
uv run pytest tests/test_daft_code_generation.py tests/test_daft_backend.py -v
```

## Next Steps

1. **Try it with your retrieval pipeline** - see the actual Daft code
2. **Analyze the operations** - understand what's happening
3. **Identify bottlenecks** - where is parallelism lacking?
4. **Hand-optimize if needed** - write a native Daft version
5. **Report findings** - share what you learned!

## Example: Complete Retrieval Pipeline Output

When you run `pipeline.show_daft_code(inputs=inputs)` on your Hebrew retrieval pipeline, you'll get output like:

```python
import daft

# ==================== Stateful Objects ====================
# encoder = <ColBERTEncoder instance>
# You need to initialize this with the same configuration

# ==================== UDF Definitions ====================

@daft.func(return_dtype=List[Python])
def load_passages_1(corpus_path: Any):
    """Load passages from corpus."""
    df = pd.read_parquet(corpus_path)
    return [Passage(uuid=row["uuid"], text=row["passage"]) for _, row in df.iterrows()]

@daft.func(return_dtype=Python)
def encode_passage_2(passage: Any, encoder: Any):
    """Encode a single passage."""
    embedding = encoder.encode(passage.text, is_query=False)
    return EncodedPassage(uuid=passage.uuid, text=passage.text, embedding=embedding)

# ... many more UDFs ...

# ==================== Pipeline Execution ====================

# Create DataFrame with input data
df = daft.from_pydict({
    "corpus_path": ['data/sample_10/corpus.parquet'],
    "examples_path": ['data/sample_10/test.parquet'],
    "model_name": ['lightonai/GTE-ModernColBERT-v1'],
    "trust_remote_code": [True],
    "index_folder": ['pylate-index'],
    "index_name": ['sample_10_index'],
    "override": [True],
    "top_k": [500],
    "rerank_k": [500],
    "rrf_k": [500],
    "ndcg_k": [20],
    "recall_k_list": [[20, 50, 100, 200, 300, 400, 500]],
})

# Add stateful objects
df = df.with_column("encoder", daft.lit(encoder))

# Load data
df = df.with_column("passages", load_passages_1(df["corpus_path"]))
df = df.with_column("queries", load_queries_2(df["examples_path"]))
df = df.with_column("ground_truths", load_ground_truths_3(df["examples_path"]))

# Create components
df = df.with_column("rrf", create_rrf_fusion_4(df["rrf_k"]))
df = df.with_column("ndcg_evaluator", create_ndcg_evaluator_5(df["ndcg_k"]))
df = df.with_column("recall_evaluator", create_recall_evaluator_6(df["recall_k_list"]))

# Map over passages to encode them
df = df.with_column("__daft_row_id_1__", daft.lit(0))
df = df.with_column("__original_passages__", df["passages"])
df = df.explode(daft.col("passages"))
df = df.with_column("encoded_passage", encode_passage_7(df["passage"], df["encoder"]))
df = df.groupby(daft.col("__daft_row_id_1__")).agg(
    daft.col("encoded_passages").list_agg().alias("encoded_passages")
)

# Build indexes
df = df.with_column("vector_index", build_vector_index_8(...))
df = df.with_column("bm25_index", build_bm25_index_9(...))
df = df.with_column("reranker", create_serializable_reranker_10(...))

# Map over queries to encode them
df = df.with_column("__daft_row_id_2__", daft.lit(0))
df = df.with_column("__original_encoded_queries__", df["encoded_queries"])
df = df.explode(daft.col("encoded_queries"))
# ... inner pipeline for retrieval ...
df = df.groupby(daft.col("__daft_row_id_2__")).agg(...)

# Evaluation
df = df.with_column("all_predictions", flatten_predictions_11(...))
df = df.with_column("ndcg_score", compute_ndcg_12(...))
df = df.with_column("recall_metrics", compute_recall_13(...))
df = df.with_column("evaluation_results", combine_evaluation_results_14(...))

# Select output columns
df = df.select(df["evaluation_results"])

# Collect results
result = df.collect()
print(result.to_pydict())
```

This shows you **exactly** what HyperNodes is doing under the hood, allowing you to:
- Identify why sequential and Daft runtimes are similar
- See where parallelism is (or isn't) happening
- Optimize the critical paths
- Write a hand-tuned native Daft version if needed

---

**The feature is ready! Try it with your retrieval pipeline and discover what's happening!** ðŸš€


# DaftBackend Pydantic Auto-Conversion

## Summary

The DaftBackend now **automatically handles Pydantic model conversions** for both inputs and outputs, making all nodes fully compatible with Daft without requiring manual dict conversion.

## What Was Implemented

### 1. Automatic Input Conversion (Dict ‚Üí Pydantic)

When using `map_over` with DaftBackend, Pydantic models in lists are exploded into dicts. The backend now automatically converts these dicts back to Pydantic models before passing them to node functions.

**Location:** `src/hypernodes/daft_backend.py` - `_convert_node_to_daft()` method

**Key Features:**
- Inspects function parameter type hints
- Detects Pydantic model parameters (single and `List[PydanticModel]`)
- Wraps functions to auto-convert dict inputs to Pydantic models
- Handles multiple input formats (dict, tuple, PyArrow struct)
- Converts lists of dicts to lists of Pydantic models
- Gracefully falls back if conversion fails

**Example:**
```python
@node(output_name="encoded_passage")
def encode_passage(passage: Passage, encoder: Encoder) -> EncodedPassage:
    """Accepts Passage (may receive dict, auto-converts to Pydantic)."""
    embedding = encoder.encode(passage.text)  # passage is guaranteed to be Pydantic
    return EncodedPassage(uuid=passage.uuid, text=passage.text, embedding=embedding)
```

### 2. Smart Pydantic Model Storage

The backend now intelligently chooses storage strategy based on Pydantic model configuration.

**Location:** `src/hypernodes/daft_backend.py` - `_infer_return_dtype()` method

**Storage Rules:**
1. **Models with `arbitrary_types_allowed`** ‚Üí Use `daft.DataType.python()`
   - This includes models with numpy arrays, torch tensors, etc.
   - Avoids PyArrow serialization issues
   - Preserves object integrity

2. **Simple Pydantic models** ‚Üí Try PyArrow struct conversion
   - More optimized when possible
   - Falls back to Python object storage if conversion fails

**Example:**
```python
class EncodedPassage(BaseModel):
    uuid: str
    text: str
    embedding: np.ndarray  # numpy array!
    model_config = {"frozen": True, "arbitrary_types_allowed": True}
    # ‚Üí Stored as Python object automatically
```

### 3. Automatic Output Conversion (Pydantic ‚Üí Dict)

This was already implemented, but now works seamlessly with the new input conversion.

**Feature:**
- When functions return Pydantic models, `.model_dump()` is called automatically
- Works with both Python object storage and PyArrow struct storage
- Maintains compatibility with downstream nodes

## How It Works

### The Complete Flow

```
1. User writes node: (Pydantic) ‚Üí (Pydantic)
                     ‚Üì
2. DaftBackend wraps function:
   - Detects Pydantic input parameters
   - Detects Pydantic return type
                     ‚Üì
3. When map_over explodes list:
   [Pydantic, Pydantic] ‚Üí explode ‚Üí Row(dict), Row(dict)
                     ‚Üì
4. Wrapper converts input:
   dict ‚Üí Pydantic (auto)
                     ‚Üì
5. User's function executes:
   Pydantic ‚Üí process ‚Üí Pydantic
                     ‚Üì
6. Wrapper converts output:
   Pydantic ‚Üí dict (via .model_dump())
                     ‚Üì
7. Daft aggregates:
   Row(dict), Row(dict) ‚Üí list_agg ‚Üí [dict, dict]
```

### Storage Strategy Decision Tree

```
Is return type Pydantic?
  ‚îú‚îÄ No ‚Üí Use standard type inference
  ‚îî‚îÄ Yes
      ‚îú‚îÄ Has arbitrary_types_allowed?
      ‚îÇ   ‚îú‚îÄ Yes ‚Üí Use daft.DataType.python()
      ‚îÇ   ‚îî‚îÄ No
      ‚îÇ       ‚îú‚îÄ pydantic_to_pyarrow available?
      ‚îÇ       ‚îÇ   ‚îú‚îÄ Yes ‚Üí Try PyArrow struct
      ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ Success ‚Üí Use PyArrow struct
      ‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ Fail ‚Üí Use Python object
      ‚îÇ       ‚îÇ   ‚îî‚îÄ No ‚Üí Use Python object
```

## Benefits

### ‚úÖ For Users

1. **Write natural code** - accept and return Pydantic models directly
2. **Full type safety** - type hints work correctly
3. **No manual conversion** - DaftBackend handles everything
4. **Works with map_over** - nested pipelines just work
5. **Handles complex types** - numpy arrays, torch tensors, etc.

### ‚úÖ For Code Quality

1. **Single source of truth** - Pydantic models define data structure
2. **Type checking** - IDEs and type checkers work correctly
3. **Validation** - Pydantic validation runs automatically
4. **Documentation** - Type hints serve as documentation
5. **Testability** - Easy to test with real Pydantic models

### ‚úÖ For Compatibility

1. **Backend-agnostic nodes** - same nodes work with LocalBackend, ModalBackend, DaftBackend
2. **Backward compatible** - existing code still works
3. **Graceful degradation** - falls back if conversion fails
4. **Flexible consumption** - downstream nodes can handle both dicts and Pydantic

## Usage Examples

### Example 1: Simple Encoding Pipeline

```python
from pydantic import BaseModel
from hypernodes import node, Pipeline
from hypernodes.daft_backend import DaftBackend

class Passage(BaseModel):
    uuid: str
    text: str
    model_config = {"frozen": True}

class EncodedPassage(BaseModel):
    uuid: str
    text: str
    embedding: np.ndarray
    model_config = {"frozen": True, "arbitrary_types_allowed": True}

@node(output_name="encoded_passage")
def encode_passage(passage: Passage, encoder: Encoder) -> EncodedPassage:
    """Clean, natural code - no manual conversion!"""
    embedding = encoder.encode(passage.text)
    return EncodedPassage(uuid=passage.uuid, text=passage.text, embedding=embedding)

# Use with map_over - just works!
encode_single = Pipeline(nodes=[encode_passage], name="encode_single")
encode_many = encode_single.as_node(
    input_mapping={"passages": "passage"},
    output_mapping={"encoded_passage": "encoded_passages"},
    map_over="passages"
)

pipeline = Pipeline(
    nodes=[load_passages, create_encoder, encode_many],
    backend=DaftBackend()
)
```

### Example 2: Flexible Consumption

For nodes that consume aggregated results (after `list_agg()`), you can write flexible code:

```python
@node(output_name="vector_index")
def build_vector_index(encoded_passages: List[Any]) -> VectorIndex:
    """Flexible: handles both dict and Pydantic inputs."""
    passages = []
    for p in encoded_passages:
        # Handle both representations
        if isinstance(p, dict):
            passages.append(EncodedPassage(**p))
        else:
            passages.append(p)
    
    return VectorIndex(passages)
```

Or even simpler - rely on DaftBackend's conversion:

```python
@node(output_name="vector_index")
def build_vector_index(encoded_passages: List[EncodedPassage]) -> VectorIndex:
    """DaftBackend auto-converts dicts to EncodedPassage!"""
    return VectorIndex(encoded_passages)
```

## Testing

All tests pass:

1. ‚úÖ **Elegant Pydantic Support Test** - `scripts/test_elegant_pydantic.py`
   - Basic Pydantic model handling
   - map_over with Pydantic models

2. ‚úÖ **Working Example Test** - `scripts/retrieval_daft_working_example.py`
   - Backward compatibility verified
   - Manual dict conversion still works

3. ‚úÖ **Original Pattern Test** - `scripts/test_original_script_pattern.py`
   - Original script pattern now works
   - No manual conversion needed

4. ‚úÖ **Hebrew Retrieval Pattern Test** - `scripts/test_hebrew_retrieval_pattern.py`
   - Complete retrieval pipeline
   - Multiple nested map operations
   - Complex dependencies

## Performance Considerations

### Python Object Storage vs PyArrow Structs

**Python Object Storage:**
- ‚úÖ Works with any Python object (numpy, torch, etc.)
- ‚úÖ No serialization/deserialization overhead for complex types
- ‚úÖ Preserves object identity and methods
- ‚ö†Ô∏è Less optimized for large-scale data processing
- ‚ö†Ô∏è No columnar access optimization

**PyArrow Structs:**
- ‚úÖ Highly optimized for columnar operations
- ‚úÖ Efficient memory layout
- ‚úÖ Fast serialization/deserialization
- ‚ö†Ô∏è Limited to PyArrow-compatible types
- ‚ö†Ô∏è Can't handle numpy arrays, torch tensors, etc.

**Our Strategy:**
- Use PyArrow structs for simple Pydantic models (when possible)
- Use Python object storage for models with `arbitrary_types_allowed`
- This balances performance and compatibility

For typical retrieval pipelines (10K-1M passages), the performance difference is negligible compared to the actual ML operations (encoding, retrieval, reranking).

## Upgrade Guide

### If You Have Existing Code with Manual Dict Conversion

**Before:**
```python
@node(output_name="encoded_passage")
def encode_passage(passage: Any, encoder: Encoder) -> dict:
    # Manual conversion
    if isinstance(passage, dict):
        passage = Passage(**passage)
    elif isinstance(passage, (list, tuple)):
        passage = Passage(uuid=passage[0], text=passage[1])
    
    embedding = encoder.encode(passage.text)
    
    # Manual dict return
    return {
        "uuid": passage.uuid,
        "text": passage.text,
        "embedding": embedding
    }
```

**After (cleaner!):**
```python
@node(output_name="encoded_passage")
def encode_passage(passage: Passage, encoder: Encoder) -> EncodedPassage:
    """Clean, simple, type-safe!"""
    embedding = encoder.encode(passage.text)
    return EncodedPassage(uuid=passage.uuid, text=passage.text, embedding=embedding)
```

### Both Versions Work!

The old version with manual conversion still works - you can upgrade incrementally.

## Requirements

No additional dependencies required! Works with:
- ‚úÖ `daft` - Required for DaftBackend
- ‚úÖ `pydantic` - Required for Pydantic models
- ‚ö†Ô∏è `pydantic-to-pyarrow` - Optional, for PyArrow struct optimization
  - If not installed, falls back to Python object storage (still works!)

## Conclusion

Your Hebrew Retrieval Pipeline nodes can now be written naturally:
- Accept Pydantic models as inputs
- Return Pydantic models as outputs
- Use `map_over` without manual conversion
- Keep full type safety

**The DaftBackend handles all conversions automatically!** üéâ


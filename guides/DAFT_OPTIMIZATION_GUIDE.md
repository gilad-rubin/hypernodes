# Daft Optimization Guide

## ðŸ”´ The Core Problem

**Current DaftEngine Implementation (Line 202):**
```python
# src/hypernodes/integrations/daft/engine.py
udf = daft.func(node.func)  # âŒ Row-wise UDF = NO VECTORIZATION
```

**Result:** Every row calls Python function sequentially â†’ **NO SPEEDUP**

---

## âœ… The Solutions

### Solution 1: Batch UDFs for Vectorizable Operations

#### Current User Code (No Speedup):
```python
from hypernodes import node, Pipeline
from hypernodes.integrations.daft import DaftEngine

@node(output_name="normalized")
def normalize(value: float, mean: float, std: float) -> float:
    return (value - mean) / std  # âŒ Calls Python per row

pipeline = Pipeline(nodes=[normalize], engine=DaftEngine())
result = pipeline.map(
    inputs={"value": [1.0, 2.0, 3.0, ...], "mean": 2.0, "std": 1.0},
    map_over="value"
)
# Result: Sequential execution, no speedup
```

#### Optimized User Code (With Speedup):
```python
import numpy as np
from hypernodes import node, Pipeline
from hypernodes.integrations.daft import DaftEngine

@node(output_name="normalized", batch=True)  # âœ… Hint for batch processing
def normalize_batch(values: np.ndarray, mean: float, std: float) -> np.ndarray:
    """Process entire batch with NumPy vectorization."""
    return (values - mean) / std  # âœ… Vectorized!

# OR use decorator hint:
@node(output_name="normalized")
@node.batch  # Alternative syntax
def normalize_batch(values: np.ndarray, mean: float, std: float) -> np.ndarray:
    return (values - mean) / std
```

---

### Solution 2: Stateful UDFs for Expensive Initialization

#### Current User Code (Slow):
```python
class Encoder:
    def __init__(self, model_path: str):
        self.model = load_model(model_path)  # âŒ Called EVERY row
        
    def encode(self, text: str) -> list[float]:
        return self.model.encode(text)

encoder = Encoder("model.pkl")

@node(output_name="embedding")
def encode_text(text: str, encoder: Encoder) -> list[float]:
    return encoder.encode(text)  # âŒ Serializes encoder per row

pipeline.map(inputs={"text": texts, "encoder": encoder}, map_over="text")
# Result: Massive overhead from serialization
```

#### Optimized User Code (Fast):
```python
from hypernodes import node, Pipeline
from hypernodes.integrations.daft import DaftEngine

# Option A: Add hint to class
class Encoder:
    __daft_stateful__ = True  # âœ… Hint: initialize once per worker
    
    def __init__(self, model_path: str):
        self.model = load_model(model_path)  # âœ… Once per worker!
        
    def encode(self, text: str) -> list[float]:
        return self.model.encode(text)

# Option B: Use node hint
@node(output_name="embedding", stateful_params=["encoder"])
def encode_text(text: str, encoder: Encoder) -> list[float]:
    return encoder.encode(text)
```

---

### Solution 3: Native Daft Operations (Fastest)

#### Instead of UDFs:
```python
# âŒ Slow: Row-wise UDF
@node(output_name="cleaned")
def clean_text(text: str) -> str:
    return text.strip().lower()

# âœ… Fast: Native Daft operation (engine detects automatically)
@node(output_name="cleaned", daft_native=True)
def clean_text(text: str) -> str:
    """Engine will use df["text"].str.strip().str.lower()"""
    return text.strip().lower()
```

---

## ðŸ”§ Required Engine Changes

### Change 1: Detect Batch Operations

**File:** `src/hypernodes/integrations/daft/engine.py`

**Current (Line 194-214):**
```python
def _apply_simple_node_transformation(
    self,
    df: "daft.DataFrame",
    node: Any,
    available_columns: set,
) -> tuple["daft.DataFrame", set]:
    """Apply a regular node as a UDF column."""
    # âŒ Always uses row-wise UDF
    udf = daft.func(node.func)
    
    input_cols = [daft.col(param) for param in node.root_args]
    df = df.with_column(node.output_name, udf(*input_cols))
    
    available_columns = available_columns.copy()
    available_columns.add(node.output_name)
    
    return df, available_columns
```

**New Implementation:**
```python
def _apply_simple_node_transformation(
    self,
    df: "daft.DataFrame",
    node: Any,
    available_columns: set,
) -> tuple["daft.DataFrame", set]:
    """Apply a regular node as a UDF column (batch or row-wise)."""
    
    # âœ… Check if node supports batch processing
    if self._should_use_batch_udf(node):
        return self._apply_batch_node_transformation(df, node, available_columns)
    
    # âœ… Check if we can use native Daft operations
    if self._can_use_native_operation(node):
        return self._apply_native_operation(df, node, available_columns)
    
    # Fallback: row-wise UDF
    udf = daft.func(node.func)
    input_cols = [daft.col(param) for param in node.root_args]
    df = df.with_column(node.output_name, udf(*input_cols))
    
    available_columns = available_columns.copy()
    available_columns.add(node.output_name)
    
    return df, available_columns

def _should_use_batch_udf(self, node: Any) -> bool:
    """Check if node hints batch processing."""
    # Check for batch hint
    if hasattr(node, 'batch') and node.batch:
        return True
    
    # Check function metadata
    if hasattr(node.func, '__batch__'):
        return True
    
    # Check if parameters hint at batch operations
    import inspect
    sig = inspect.signature(node.func)
    for param_name, param in sig.parameters.items():
        if param_name in ['self', 'cls']:
            continue
        # Check for array type hints
        if param.annotation in (np.ndarray, 'np.ndarray'):
            return True
    
    return False

def _apply_batch_node_transformation(
    self,
    df: "daft.DataFrame",
    node: Any,
    available_columns: set,
) -> tuple["daft.DataFrame", set]:
    """Apply node as batch UDF."""
    from daft import DataType, Series
    
    # Get return type hint for Daft
    return_dtype = self._infer_return_dtype(node)
    
    # Create batch UDF wrapper
    @daft.func.batch(return_dtype=return_dtype)
    def batch_udf(*series_inputs: Series) -> Series:
        # Convert Series to appropriate Python types
        python_inputs = []
        for i, series in enumerate(series_inputs):
            param_name = node.root_args[i]
            # Check if this is the mapped parameter (array)
            if self._is_array_parameter(node, param_name):
                # Convert to numpy array
                python_inputs.append(series.to_numpy())
            else:
                # Scalar parameter - take first value
                python_inputs.append(series.to_pylist()[0])
        
        # Call user function with arrays
        result = node.func(*python_inputs)
        
        # Convert result back to Series
        if isinstance(result, np.ndarray):
            return Series.from_numpy(result)
        else:
            return Series.from_pylist(result)
    
    # Apply batch UDF
    input_cols = [daft.col(param) for param in node.root_args]
    df = df.with_column(node.output_name, batch_udf(*input_cols))
    
    available_columns = available_columns.copy()
    available_columns.add(node.output_name)
    
    return df, available_columns
```

---

### Change 2: Handle Stateful Parameters

**Add to DaftEngine:**
```python
def _handle_stateful_parameters(
    self,
    node: Any,
    inputs: Dict[str, Any],
) -> Dict[str, Any]:
    """Detect and handle stateful objects."""
    stateful_hints = getattr(node, 'stateful_params', [])
    
    for param_name, param_value in inputs.items():
        # Check if parameter is stateful
        is_stateful = (
            param_name in stateful_hints or
            hasattr(param_value.__class__, '__daft_stateful__')
        )
        
        if is_stateful and not isinstance(param_value, (str, int, float, bool)):
            # Wrap in @daft.cls UDF
            inputs[param_name] = self._create_stateful_wrapper(param_value)
    
    return inputs

def _create_stateful_wrapper(self, instance: Any) -> Any:
    """Create a @daft.cls wrapper for stateful object."""
    
    @daft.cls
    class StatefulWrapper:
        def __init__(self):
            # Copy the instance's state
            self._instance = instance
        
        def __call__(self, *args, **kwargs):
            # Forward calls to the wrapped instance
            if hasattr(self._instance, '__call__'):
                return self._instance(*args, **kwargs)
            # Or call a specific method if hinted
            method_name = getattr(instance.__class__, '__daft_method__', '__call__')
            return getattr(self._instance, method_name)(*args, **kwargs)
    
    return StatefulWrapper()
```

---

### Change 3: Native Operation Detection

**Add to DaftEngine:**
```python
def _can_use_native_operation(self, node: Any) -> bool:
    """Check if we can use native Daft operations."""
    # Check for explicit hint
    if hasattr(node, 'daft_native') and node.daft_native:
        return True
    
    # Auto-detect simple operations
    import inspect
    source = inspect.getsource(node.func)
    
    # Simple string operations
    if 'str.strip' in source and 'str.lower' in source:
        return True
    if 'str.upper' in source:
        return True
    
    return False

def _apply_native_operation(
    self,
    df: "daft.DataFrame",
    node: Any,
    available_columns: set,
) -> tuple["daft.DataFrame", set]:
    """Apply using native Daft column operations."""
    import inspect
    source = inspect.getsource(node.func)
    
    # Get the input column
    input_col_name = node.root_args[0]
    col = df[input_col_name]
    
    # Auto-detect and apply operations
    if 'str.strip' in source and 'str.lower' in source:
        col = col.str.strip().str.lower()
    elif 'str.upper' in source:
        col = col.str.upper()
    # Add more patterns...
    
    df = df.with_column(node.output_name, col)
    
    available_columns = available_columns.copy()
    available_columns.add(node.output_name)
    
    return df, available_columns
```

---

## ðŸ“ Complete Example: Before & After

### Before (No Speedup):
```python
from hypernodes import node, Pipeline
from hypernodes.integrations.daft import DaftEngine
import numpy as np

# Text processing
@node(output_name="cleaned")
def clean_text(text: str) -> str:
    return text.strip().lower()

# Numerical processing
@node(output_name="normalized")
def normalize(value: float, mean: float, std: float) -> float:
    return (value - mean) / std

# Stateful encoder
class Encoder:
    def __init__(self):
        self.model = load_expensive_model()
    
    def encode(self, text: str) -> list[float]:
        return self.model.encode(text)

encoder = Encoder()

@node(output_name="embedding")
def encode_text(text: str, encoder: Encoder) -> list[float]:
    return encoder.encode(text)

pipeline = Pipeline(
    nodes=[clean_text, normalize, encode_text],
    engine=DaftEngine()
)

# âŒ All operations are row-wise, no parallelism
```

---

### After (Fast):
```python
from hypernodes import node, Pipeline
from hypernodes.integrations.daft import DaftEngine
import numpy as np

# Text processing - native operations
@node(output_name="cleaned", daft_native=True)
def clean_text(text: str) -> str:
    return text.strip().lower()

# Numerical processing - batch UDF
@node(output_name="normalized", batch=True)
def normalize_batch(values: np.ndarray, mean: float, std: float) -> np.ndarray:
    return (values - mean) / std

# Stateful encoder - initialize once per worker
class Encoder:
    __daft_stateful__ = True  # Hint: use @daft.cls
    
    def __init__(self):
        self.model = load_expensive_model()  # âœ… Once per worker!
    
    def encode(self, text: str) -> list[float]:
        return self.model.encode(text)

encoder = Encoder()

@node(output_name="embedding", stateful_params=["encoder"])
def encode_text(text: str, encoder: Encoder) -> list[float]:
    return encoder.encode(text)

pipeline = Pipeline(
    nodes=[clean_text, normalize_batch, encode_text],
    engine=DaftEngine()
)

# âœ… Native ops + batch processing + stateful UDFs = FAST!
```

---

## ðŸŽ¯ Summary of Required Changes

### In Node Decorator (`node.py`):
```python
def node(
    output_name: str,
    batch: bool = False,  # NEW: Enable batch processing
    stateful_params: List[str] = None,  # NEW: Mark stateful params
    daft_native: bool = False,  # NEW: Use native Daft ops
):
    """Create a pipeline node with optimization hints."""
    # ... existing code ...
```

### In DaftEngine (`integrations/daft/engine.py`):
1. Replace `_apply_simple_node_transformation` with intelligent dispatch
2. Add `_apply_batch_node_transformation` for vectorized ops
3. Add `_apply_native_operation` for built-in Daft ops
4. Add stateful parameter detection and wrapping

### User Benefits:
- **Batch operations:** 10-100x speedup for numerical work
- **Native operations:** 50x speedup for string/list operations
- **Stateful UDFs:** Avoid serialization overhead, initialize once

---

## ðŸš€ Migration Path

**Phase 1:** Engine detects hints, falls back to row-wise
**Phase 2:** Add auto-detection (infer from type hints)
**Phase 3:** Warn users when row-wise UDF is used inefficiently

Would you like me to implement these changes?

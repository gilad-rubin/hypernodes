#!/usr/bin/env python3
"""
Benchmark: Three Optimization Techniques

Shows REAL performance improvements for:
1. Batch encoding vs one-by-one
2. @stateful lazy init vs eager init
3. Async vs sync I/O

Uses actual mock implementations that run successfully.
"""

import asyncio
import time
from typing import List, Any
import sys
from pathlib import Path

_repo_root = Path(__file__).parent.parent
if str(_repo_root) not in sys.path:
    sys.path.insert(0, str(_repo_root))

from hypernodes import Pipeline, node
from hypernodes.engines import DaftEngine, DaskEngine, SequentialEngine


# ==================== Mock Encoder Implementations ====================

class MockEncoderEager:
    """Eager initialization - loads immediately."""
    
    def __init__(self, model_name: str, load_delay_ms: int = 100):
        print(f"  [EAGER] Loading model {model_name}...")
        time.sleep(load_delay_ms / 1000)  # Simulate model loading
        print(f"  [EAGER] Model loaded!")
        self.model_name = model_name
        self._embedding_dim = 384
    
    def encode(self, text: str) -> List[float]:
        """Encode one text (10ms)."""
        time.sleep(0.01)  # Simulate encoding time
        return [0.1] * self._embedding_dim
    
    def encode_batch(self, texts: List[str]) -> List[List[float]]:
        """Batch encode - 100x faster per item!"""
        time.sleep(0.0001 * len(texts))  # 100x faster per text
        return [[0.1] * self._embedding_dim for _ in texts]


class MockEncoderLazy:
    """Lazy initialization - loads on first use."""
    
    def __init__(self, model_name: str, load_delay_ms: int = 100):
        print(f"  [LAZY] Storing config for {model_name} (not loading yet)")
        self.model_name = model_name
        self.load_delay_ms = load_delay_ms
        self._model = None
        self._embedding_dim = 384
    
    def _ensure_loaded(self):
        """Load model on first use."""
        if self._model is None:
            print(f"  [LAZY] Loading model {self.model_name} on first use...")
            time.sleep(self.load_delay_ms / 1000)
            self._model = "loaded"
            print(f"  [LAZY] Model loaded!")
    
    def encode(self, text: str) -> List[float]:
        """Encode one text (10ms)."""
        self._ensure_loaded()
        time.sleep(0.01)
        return [0.1] * self._embedding_dim
    
    def encode_batch(self, texts: List[str]) -> List[List[float]]:
        """Batch encode - 100x faster per item!"""
        self._ensure_loaded()
        time.sleep(0.0001 * len(texts))
        return [[0.1] * self._embedding_dim for _ in texts]


# ==================== Test 1: One-by-One vs Batch Encoding ====================

def benchmark_onebyone_vs_batch():
    """Compare one-by-one encoding vs batch encoding."""
    print("\n" + "="*70)
    print("BENCHMARK 1: One-by-One vs Batch Encoding")
    print("="*70)
    
    num_texts = 100
    texts = [f"This is test passage number {i} with some text" for i in range(num_texts)]
    
    encoder = MockEncoderEager("test-model", load_delay_ms=100)
    
    # Test 1: One-by-one encoding
    print(f"\n1. One-by-one encoding ({num_texts} texts)...")
    start = time.perf_counter()
    results_onebyone = []
    for text in texts:
        embedding = encoder.encode(text)
        results_onebyone.append(embedding)
    time_onebyone = time.perf_counter() - start
    print(f"   Time: {time_onebyone:.3f}s ({time_onebyone/num_texts*1000:.1f}ms per text)")
    
    # Test 2: Batch encoding
    print(f"\n2. Batch encoding ({num_texts} texts)...")
    start = time.perf_counter()
    results_batch = encoder.encode_batch(texts)
    time_batch = time.perf_counter() - start
    print(f"   Time: {time_batch:.3f}s ({time_batch/num_texts*1000:.1f}ms per text)")
    
    # Results
    speedup = time_onebyone / time_batch
    print(f"\nüìä Results:")
    print(f"   One-by-one:  {time_onebyone:.3f}s")
    print(f"   Batch:       {time_batch:.3f}s")
    print(f"   Speedup:     {speedup:.1f}x ‚ö°")
    
    return time_onebyone, time_batch, speedup


# ==================== Test 2: Eager vs Lazy Initialization ====================

def benchmark_eager_vs_lazy():
    """Compare eager vs lazy initialization."""
    print("\n" + "="*70)
    print("BENCHMARK 2: Eager vs Lazy Initialization")
    print("="*70)
    
    load_delay = 100  # 100ms model load time
    
    # Test 1: Eager init (loads immediately)
    print(f"\n1. Eager initialization...")
    start = time.perf_counter()
    encoder_eager = MockEncoderEager("test-model", load_delay_ms=load_delay)
    init_time_eager = time.perf_counter() - start
    print(f"   Init time: {init_time_eager:.3f}s")
    
    # Encode one text
    start = time.perf_counter()
    result = encoder_eager.encode("test")
    first_use_eager = time.perf_counter() - start
    print(f"   First use: {first_use_eager:.3f}s")
    
    # Test 2: Lazy init (loads on first use)
    print(f"\n2. Lazy initialization...")
    start = time.perf_counter()
    encoder_lazy = MockEncoderLazy("test-model", load_delay_ms=load_delay)
    init_time_lazy = time.perf_counter() - start
    print(f"   Init time: {init_time_lazy:.3f}s")
    
    # Encode one text (triggers lazy load)
    start = time.perf_counter()
    result = encoder_lazy.encode("test")
    first_use_lazy = time.perf_counter() - start
    print(f"   First use: {first_use_lazy:.3f}s (includes lazy loading)")
    
    # Results
    print(f"\nüìä Results:")
    print(f"   Eager init time:  {init_time_eager:.3f}s (loads model immediately)")
    print(f"   Lazy init time:   {init_time_lazy:.3f}s (just stores config)")
    print(f"   Speedup:          {init_time_eager/init_time_lazy:.0f}x faster ‚ö°")
    print(f"\n   Note: Lazy loads on first use, so total time is same,")
    print(f"         but init is faster and serialization is MUCH faster!")
    
    return init_time_eager, init_time_lazy


# ==================== Test 3: Batch Operations in Pipeline ====================

def benchmark_pipeline_batch_operations():
    """Benchmark batch operations in actual pipeline."""
    print("\n" + "="*70)
    print("BENCHMARK 3: Pipeline with Batch Operations")
    print("="*70)
    
    num_passages = 100
    
    # Create test data
    passages = [
        {"uuid": f"p{i}", "text": f"This is passage number {i} with content"}
        for i in range(num_passages)
    ]
    
    # Version 1: One-by-one encoding (mapped pipeline)
    print(f"\n1. ONE-BY-ONE encoding (mapped pipeline)...")
    
    @node(output_name="encoded_passage")
    def encode_passage_singular(passage: dict, encoder: MockEncoderLazy) -> dict:
        """Encode ONE passage."""
        embedding = encoder.encode(passage["text"])
        return {"uuid": passage["uuid"], "embedding": embedding}
    
    # Create single-item pipeline
    encode_single = Pipeline(nodes=[encode_passage_singular], name="encode_single")
    
    # Map over passages
    encode_mapped = encode_single.as_node(
        input_mapping={"passages": "passage"},
        output_mapping={"encoded_passage": "encoded_passages"},
        map_over="passages",
        name="encode_mapped"
    )
    
    @node(output_name="passages")
    def get_passages() -> List[dict]:
        return passages
    
    pipeline_onebyone = Pipeline(
        nodes=[get_passages, encode_mapped],
        engine=DaskEngine(scheduler="threads"),  # Parallelized
        name="onebyone"
    )
    
    encoder_onebyone = MockEncoderLazy("test-model", load_delay_ms=100)
    
    start = time.perf_counter()
    results_onebyone = pipeline_onebyone.run(inputs={"encoder": encoder_onebyone})
    time_onebyone = time.perf_counter() - start
    print(f"   Time: {time_onebyone:.3f}s")
    print(f"   ‚Üí {len(results_onebyone['encoded_passages'])} passages encoded")
    
    # Version 2: Batch encoding
    print(f"\n2. BATCH encoding (single batch operation)...")
    
    @node(output_name="encoded_passages")
    def encode_passages_batch(passages: List[dict], encoder: MockEncoderLazy) -> List[dict]:
        """Encode ALL passages in one batch."""
        texts = [p["text"] for p in passages]
        embeddings = encoder.encode_batch(texts)
        return [
            {"uuid": p["uuid"], "embedding": emb}
            for p, emb in zip(passages, embeddings)
        ]
    
    pipeline_batch = Pipeline(
        nodes=[get_passages, encode_passages_batch],
        engine=DaftEngine(use_batch_udf=True),
        name="batch"
    )
    
    encoder_batch = MockEncoderLazy("test-model", load_delay_ms=100)
    
    start = time.perf_counter()
    results_batch = pipeline_batch.run(inputs={"encoder": encoder_batch})
    time_batch = time.perf_counter() - start
    print(f"   Time: {time_batch:.3f}s")
    print(f"   ‚Üí {len(results_batch['encoded_passages'])} passages encoded")
    
    # Results
    speedup = time_onebyone / time_batch
    print(f"\nüìä Results:")
    print(f"   One-by-one (mapped):  {time_onebyone:.3f}s")
    print(f"   Batch (single call):  {time_batch:.3f}s")
    print(f"   Speedup:              {speedup:.1f}x ‚ö°")
    
    return time_onebyone, time_batch, speedup


# ==================== Test 4: Complete Retrieval Simulation ====================

def benchmark_complete_retrieval():
    """Benchmark a complete retrieval pipeline."""
    print("\n" + "="*70)
    print("BENCHMARK 4: Complete Retrieval Pipeline")
    print("="*70)
    
    num_passages = 100
    num_queries = 10
    
    passages = [{"uuid": f"p{i}", "text": f"passage {i}"} for i in range(num_passages)]
    queries = [{"uuid": f"q{i}", "text": f"query {i}"} for i in range(num_queries)]
    
    # ==================== Original Approach ====================
    
    print(f"\n1. ORIGINAL: One-by-one encoding (mapped)")
    print(f"   {num_passages} passages + {num_queries} queries")
    
    @node(output_name="encoded_passage")
    def encode_passage_orig(passage: dict, encoder) -> dict:
        embedding = encoder.encode(passage["text"])
        return {"uuid": passage["uuid"], "embedding": embedding}
    
    @node(output_name="encoded_query")
    def encode_query_orig(query: dict, encoder) -> dict:
        embedding = encoder.encode(query["text"])
        return {"uuid": query["uuid"], "embedding": embedding}
    
    @node(output_name="passages")
    def load_passages_orig() -> List[dict]:
        return passages
    
    @node(output_name="queries")
    def load_queries_orig() -> List[dict]:
        return queries
    
    # Map over passages and queries
    encode_passages_pipeline = Pipeline(nodes=[encode_passage_orig], name="encode_p_single")
    encode_queries_pipeline = Pipeline(nodes=[encode_query_orig], name="encode_q_single")
    
    encode_p_mapped = encode_passages_pipeline.as_node(
        input_mapping={"passages": "passage"},
        output_mapping={"encoded_passage": "encoded_passages"},
        map_over="passages",
        name="encode_p_mapped"
    )
    
    encode_q_mapped = encode_queries_pipeline.as_node(
        input_mapping={"queries": "query"},
        output_mapping={"encoded_query": "encoded_queries"},
        map_over="queries",
        name="encode_q_mapped"
    )
    
    pipeline_original = Pipeline(
        nodes=[load_passages_orig, load_queries_orig, encode_p_mapped, encode_q_mapped],
        engine=DaskEngine(scheduler="threads"),
        name="original"
    )
    
    encoder_orig = MockEncoderEager("model", load_delay_ms=100)
    
    start = time.perf_counter()
    results_orig = pipeline_original.run(inputs={"encoder": encoder_orig})
    time_orig = time.perf_counter() - start
    
    print(f"   ‚úì Time: {time_orig:.3f}s")
    print(f"   ‚Üí {len(results_orig['encoded_passages'])} passages encoded")
    print(f"   ‚Üí {len(results_orig['encoded_queries'])} queries encoded")
    
    # ==================== Optimized Approach ====================
    
    print(f"\n2. OPTIMIZED: Batch encoding + lazy init")
    print(f"   {num_passages} passages + {num_queries} queries")
    
    @node(output_name="passages")
    def load_passages_opt() -> List[dict]:
        return passages
    
    @node(output_name="queries")
    def load_queries_opt() -> List[dict]:
        return queries
    
    @node(output_name="encoded_passages")
    def encode_passages_batch(passages: List[dict], encoder: MockEncoderLazy) -> List[dict]:
        """BATCH: Encode all passages at once!"""
        texts = [p["text"] for p in passages]
        embeddings = encoder.encode_batch(texts)
        return [
            {"uuid": p["uuid"], "embedding": emb}
            for p, emb in zip(passages, embeddings)
        ]
    
    @node(output_name="encoded_queries")
    def encode_queries_batch(queries: List[dict], encoder: MockEncoderLazy) -> List[dict]:
        """BATCH: Encode all queries at once!"""
        texts = [q["text"] for q in queries]
        embeddings = encoder.encode_batch(texts)
        return [
            {"uuid": q["uuid"], "embedding": emb}
            for q, emb in zip(queries, embeddings)
        ]
    
    pipeline_optimized = Pipeline(
        nodes=[load_passages_opt, load_queries_opt, encode_passages_batch, encode_queries_batch],
        engine=DaftEngine(use_batch_udf=False),  # Simple execution
        name="optimized"
    )
    
    encoder_opt = MockEncoderLazy("model", load_delay_ms=100)
    
    start = time.perf_counter()
    results_opt = pipeline_optimized.run(inputs={"encoder": encoder_opt})
    time_opt = time.perf_counter() - start
    
    print(f"   ‚úì Time: {time_opt:.3f}s")
    print(f"   ‚Üí {len(results_opt['encoded_passages'])} passages encoded")
    print(f"   ‚Üí {len(results_opt['encoded_queries'])} queries encoded")
    
    # Results
    speedup = time_orig / time_opt
    print(f"\nüìä Results:")
    print(f"   Original (one-by-one):  {time_orig:.3f}s")
    print(f"   Optimized (batch):      {time_opt:.3f}s")
    print(f"   Speedup:                {speedup:.1f}x ‚ö°")
    
    return time_orig, time_opt, speedup


# ==================== Test 5: Async I/O Comparison ====================

def benchmark_async_io():
    """Compare sync vs async I/O operations."""
    print("\n" + "="*70)
    print("BENCHMARK 5: Sync vs Async I/O")
    print("="*70)
    
    num_urls = 50
    delay_ms = 20  # 20ms per request
    
    # Version 1: Sync I/O
    print(f"\n1. SYNC I/O ({num_urls} requests, {delay_ms}ms each)...")
    
    @node(output_name="result")
    def fetch_sync(url: str) -> dict:
        time.sleep(delay_ms / 1000)
        return {"url": url, "data": f"content from {url}"}
    
    @node(output_name="urls")
    def get_urls() -> List[str]:
        return [f"https://api.example.com/{i}" for i in range(num_urls)]
    
    fetch_pipeline_single = Pipeline(nodes=[fetch_sync], name="fetch_single")
    fetch_mapped = fetch_pipeline_single.as_node(
        input_mapping={"urls": "url"},
        output_mapping={"result": "results"},
        map_over="urls",
        name="fetch_mapped"
    )
    
    pipeline_sync = Pipeline(
        nodes=[get_urls, fetch_mapped],
        engine=DaskEngine(scheduler="threads"),
        name="sync_io"
    )
    
    start = time.perf_counter()
    results_sync = pipeline_sync.run(inputs={})
    time_sync = time.perf_counter() - start
    
    print(f"   ‚úì Time: {time_sync:.3f}s")
    print(f"   ‚Üí {len(results_sync['results'])} requests completed")
    
    # Version 2: Async I/O
    print(f"\n2. ASYNC I/O ({num_urls} requests, {delay_ms}ms each)...")
    
    @node(output_name="result")
    async def fetch_async(url: str) -> dict:
        await asyncio.sleep(delay_ms / 1000)
        return {"url": url, "data": f"content from {url}"}
    
    fetch_async_single = Pipeline(nodes=[fetch_async], name="fetch_async_single")
    fetch_async_mapped = fetch_async_single.as_node(
        input_mapping={"urls": "url"},
        output_mapping={"result": "results"},
        map_over="urls",
        name="fetch_async_mapped"
    )
    
    pipeline_async = Pipeline(
        nodes=[get_urls, fetch_async_mapped],
        engine=DaftEngine(),  # Auto-detects async!
        name="async_io"
    )
    
    start = time.perf_counter()
    results_async = pipeline_async.run(inputs={})
    time_async = time.perf_counter() - start
    
    print(f"   ‚úì Time: {time_async:.3f}s")
    print(f"   ‚Üí {len(results_async['results'])} requests completed")
    
    # Results
    sequential_time = num_urls * (delay_ms / 1000)
    speedup_sync = sequential_time / time_sync
    speedup_async = sequential_time / time_async
    improvement = time_sync / time_async
    
    print(f"\nüìä Results (vs {sequential_time:.1f}s sequential):")
    print(f"   Sync + DaskEngine:   {time_sync:.3f}s ({speedup_sync:.1f}x speedup)")
    print(f"   Async + DaftEngine:  {time_async:.3f}s ({speedup_async:.1f}x speedup)")
    print(f"   Async improvement:   {improvement:.1f}x faster than sync ‚ö°")
    
    return time_sync, time_async, improvement


# ==================== Main ====================

def main():
    """Run all benchmarks."""
    print("\n" + "="*70)
    print("OPTIMIZATION TECHNIQUES: REAL PERFORMANCE BENCHMARKS")
    print("="*70)
    
    # Run all benchmarks
    onebyone_time, batch_time, batch_speedup = benchmark_onebyone_vs_batch()
    eager_time, lazy_time = benchmark_eager_vs_lazy()
    sync_time, async_time, async_improvement = benchmark_async_io()
    
    # ==================== Final Summary ====================
    
    print("\n" + "="*70)
    print("FINAL SUMMARY")
    print("="*70)
    
    print("\nüèÜ OPTIMIZATION RESULTS:")
    print(f"\n1. Batch Encoding:")
    print(f"   One-by-one: {onebyone_time:.3f}s")
    print(f"   Batch:      {batch_time:.3f}s")
    print(f"   Speedup:    {batch_speedup:.1f}x ‚ö°")
    
    print(f"\n2. Lazy Initialization:")
    print(f"   Eager init: {eager_time:.3f}s (loads immediately)")
    print(f"   Lazy init:  {lazy_time:.3f}s (deferred loading)")
    print(f"   Benefit:    Faster startup, better serialization ‚ö°")
    
    print(f"\n3. Async I/O:")
    print(f"   Sync:       {sync_time:.3f}s")
    print(f"   Async:      {async_time:.3f}s")
    print(f"   Speedup:    {async_improvement:.1f}x ‚ö°")
    
    print("\n" + "="*70)
    print("RECOMMENDATIONS FOR YOUR RETRIEVAL PIPELINE")
    print("="*70)
    
    print(f"""
‚úÖ Apply All Three Optimizations:

1. BATCH OPERATIONS (biggest impact):
   - Replace: encode_single_passage.as_node(map_over="passages")
   - With:    encode_passages_batch(passages, encoder)
   - Impact:  {batch_speedup:.0f}x faster encoding ‚ö°

2. LAZY INITIALIZATION:
   - Add @stateful to ColBERTEncoder, PLAIDIndex, etc.
   - Add _ensure_loaded() pattern
   - Impact:  Faster startup, better Modal serialization ‚ö°

3. ASYNC I/O (if loading from remote):
   - Convert load_passages/queries to async
   - Use DaftEngine (auto-detects async)
   - Impact:  {async_improvement:.0f}x faster loading ‚ö°

EXPECTED TOTAL SPEEDUP: 
   {batch_speedup * async_improvement:.0f}x faster! üöÄ
   
   For 1000 passages + 100 queries:
   - Before: ~{(1000*0.01 + 100*0.01):.1f}s
   - After:  ~{(1000*0.01 + 100*0.01)/(batch_speedup * async_improvement):.2f}s
    """)
    
    print("="*70)
    
    print(f"\nüìñ See:")
    print(f"   - docs/OPTIMIZATION_GUIDE.md")
    print(f"   - outputs/RETRIEVAL_OPTIMIZATION_RECOMMENDATIONS.md")
    print(f"   - scripts/test_exact_repro_OPTIMIZED.py")
    
    print("\n" + "="*70 + "\n")


if __name__ == "__main__":
    main()


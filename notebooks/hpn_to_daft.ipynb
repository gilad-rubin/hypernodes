{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b337144",
   "metadata": {},
   "source": [
    "# HyperNodes vs Daft Pipeline Performance Benchmark\n",
    "\n",
    "Comparing hypernodes pipelines with equivalent daft pipelines.\n",
    "Testing the same 3-function pipeline (A â†’ B â†’ C) with progressively larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49cf65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import daft\n",
    "from daft import DataType, Series\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow as pa\n",
    "from hypernodes import Pipeline, node\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97448663",
   "metadata": {},
   "source": [
    "## Step 1: Define Base Functions\n",
    "\n",
    "Three functions that form our pipeline: A â†’ B â†’ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47809395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base functions test:\n",
      "  func_a(3) = 9\n",
      "  func_b(9) = 19\n",
      "  func_c(19) = 4.3589\n"
     ]
    }
   ],
   "source": [
    "# Base functions - used by ALL implementations\n",
    "def func_a(x: float) -> float:\n",
    "    \"\"\"Function A: Square the input\"\"\"\n",
    "    return x**2\n",
    "\n",
    "\n",
    "def func_b(a_result: float) -> float:\n",
    "    \"\"\"Function B: Add 10 to the result\"\"\"\n",
    "    return a_result + 10\n",
    "\n",
    "\n",
    "def func_c(b_result: float) -> float:\n",
    "    \"\"\"Function C: Take square root\"\"\"\n",
    "    return b_result**0.5\n",
    "\n",
    "\n",
    "# Verify the functions work\n",
    "print(\"Base functions test:\")\n",
    "print(f\"  func_a(3) = {func_a(3)}\")  # 9\n",
    "print(f\"  func_b(9) = {func_b(9)}\")  # 19\n",
    "print(f\"  func_c(19) = {func_c(19):.4f}\")  # 4.3589"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7a33c",
   "metadata": {},
   "source": [
    "## Benchmark 1: Single Row (test_value = 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d91ca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Row: HyperNodes\n",
      "  Result: {'a_result': 9.0, 'b_result': 19.0, 'c_result': 4.358898943540674}\n",
      "  Avg: 0.0156ms (Â±0.0035ms)\n"
     ]
    }
   ],
   "source": [
    "# HyperNodes: Single row execution\n",
    "@node(output_name=\"a_result\")\n",
    "def node_a(x: float) -> float:\n",
    "    return func_a(x)\n",
    "\n",
    "\n",
    "@node(output_name=\"b_result\")\n",
    "def node_b(a_result: float) -> float:\n",
    "    return func_b(a_result)\n",
    "\n",
    "\n",
    "@node(output_name=\"c_result\")\n",
    "def node_c(b_result: float) -> float:\n",
    "    return func_c(b_result)\n",
    "\n",
    "\n",
    "test_value = 3.0\n",
    "hpn_pipeline = Pipeline(nodes=[node_a, node_b, node_c])\n",
    "\n",
    "# Warm-up\n",
    "hpn_pipeline.run(inputs={\"x\": test_value})\n",
    "\n",
    "# Benchmark\n",
    "times_hpn = []\n",
    "for _ in range(10):\n",
    "    start = time.perf_counter()\n",
    "    result = hpn_pipeline.run(inputs={\"x\": test_value})\n",
    "    times_hpn.append((time.perf_counter() - start) * 1000)\n",
    "\n",
    "avg_hpn = np.mean(times_hpn)\n",
    "std_hpn = np.std(times_hpn)\n",
    "\n",
    "print(\"Single Row: HyperNodes\")\n",
    "print(f\"  Result: {result}\")\n",
    "print(f\"  Avg: {avg_hpn:.4f}ms (Â±{std_hpn:.4f}ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5106ae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Row: Daft (Row-wise)\n",
      "  Result: {'x': [3.0], 'a_result': [9.0], 'b_result': [19.0], 'c_result': [4.358898943540674]}\n",
      "  Avg: 2.7492ms (Â±0.7415ms)\n",
      "  vs HyperNodes: 176.14x\n"
     ]
    }
   ],
   "source": [
    "# Daft: Single row (Row-wise UDFs)\n",
    "@daft.func(return_dtype=DataType.float64())\n",
    "def daft_func_a_rowwise(x: float) -> float:\n",
    "    return func_a(x)\n",
    "\n",
    "\n",
    "@daft.func(return_dtype=DataType.float64())\n",
    "def daft_func_b_rowwise(a_result: float) -> float:\n",
    "    return func_b(a_result)\n",
    "\n",
    "\n",
    "@daft.func(return_dtype=DataType.float64())\n",
    "def daft_func_c_rowwise(b_result: float) -> float:\n",
    "    return func_c(b_result)\n",
    "\n",
    "\n",
    "df_single = daft.from_pydict({\"x\": [test_value]})\n",
    "\n",
    "# Warm-up\n",
    "(\n",
    "    df_single.with_column(\"a_result\", daft_func_a_rowwise(daft.col(\"x\")))\n",
    "    .with_column(\"b_result\", daft_func_b_rowwise(daft.col(\"a_result\")))\n",
    "    .with_column(\"c_result\", daft_func_c_rowwise(daft.col(\"b_result\")))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Benchmark\n",
    "times_daft_rowwise = []\n",
    "for _ in range(10):\n",
    "    start = time.perf_counter()\n",
    "    result = (\n",
    "        df_single.with_column(\"a_result\", daft_func_a_rowwise(daft.col(\"x\")))\n",
    "        .with_column(\"b_result\", daft_func_b_rowwise(daft.col(\"a_result\")))\n",
    "        .with_column(\"c_result\", daft_func_c_rowwise(daft.col(\"b_result\")))\n",
    "        .collect()\n",
    "    )\n",
    "    times_daft_rowwise.append((time.perf_counter() - start) * 1000)\n",
    "\n",
    "avg_daft_rowwise = np.mean(times_daft_rowwise)\n",
    "std_daft_rowwise = np.std(times_daft_rowwise)\n",
    "\n",
    "print(\"Single Row: Daft (Row-wise)\")\n",
    "print(f\"  Result: {result.to_pydict()}\")\n",
    "print(f\"  Avg: {avg_daft_rowwise:.4f}ms (Â±{std_daft_rowwise:.4f}ms)\")\n",
    "print(f\"  vs HyperNodes: {avg_daft_rowwise / avg_hpn:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7002bc",
   "metadata": {},
   "source": [
    "## Benchmark 2: 100K Rows - Same Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c20c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 100,000 rows\n",
      "Sample input: [4.370861069626263, 9.556428757689245, 7.587945476302646]\n",
      "\n",
      "100K Rows: HyperNodes\n",
      "  Time: 1528.19ms\n",
      "  Per-row: 15.282Î¼s\n",
      "  Sample results: [{'a_result': 19.104426489974436, 'b_result': 29.104426489974436, 'c_result': 5.394851850604837}, {'a_result': 91.32533060079001, 'b_result': 101.32533060079001, 'c_result': 10.066048410413593}]\n"
     ]
    }
   ],
   "source": [
    "# Generate test data: 100K rows\n",
    "n_rows = 100_000\n",
    "test_data = np.random.uniform(1, 10, n_rows).tolist()\n",
    "\n",
    "print(f\"Testing with {n_rows:,} rows\")\n",
    "print(f\"Sample input: {test_data[:3]}\\n\")\n",
    "\n",
    "# HyperNodes: map over 100K rows (using same functions)\n",
    "start = time.perf_counter()\n",
    "hpn_results = hpn_pipeline.map(inputs={\"x\": test_data}, map_over=\"x\")\n",
    "time_hpn_100k = (time.perf_counter() - start) * 1000\n",
    "\n",
    "print(\"100K Rows: HyperNodes\")\n",
    "print(f\"  Time: {time_hpn_100k:.2f}ms\")\n",
    "print(f\"  Per-row: {time_hpn_100k * 1000 / n_rows:.3f}Î¼s\")\n",
    "print(f\"  Sample results: {hpn_results[:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a125d1",
   "metadata": {},
   "source": [
    "## Benchmark 3: 100K Rows - Batch UDFs (Optimization)\n",
    "\n",
    "Now using @daft.func.batch to vectorize the operations - same base functions wrapped differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf80ca5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Row: HyperNodes\n",
      "  Result: {'a_result': 9.0, 'b_result': 19.0, 'c_result': 4.358898943540674}\n",
      "  Avg: 0.0161ms (Â±0.0038ms)\n",
      "Single Row: Dask (delayed DAG)\n",
      "  Result: 4.358898943540674\n",
      "  Avg: 1.1703ms (Â±0.3556ms)\n",
      "  vs HyperNodes: 72.78x\n",
      "\n",
      "Testing with 100,000 rows\n",
      "Sample input: [3.543291703069916, 5.128089314593314, 1.8929394812767522]\n",
      "\n",
      "100K Rows: HyperNodes\n",
      "  Time: 1573.80ms\n",
      "  Per-row: 15.738Î¼s\n",
      "  Sample results: [{'a_result': 12.554916093044106, 'b_result': 22.554916093044106, 'c_result': 4.749201626909949}, {'a_result': 26.297300018446126, 'b_result': 36.29730001844612, 'c_result': 6.024724061602002}]\n",
      "\n",
      "100K Rows: Dask (Bag, row-wise)\n",
      "  Time: 340.35ms\n",
      "  Per-row: 3.404Î¼s\n",
      "  Sample results: [4.749201626909949, 6.024724061602002]\n",
      "  vs HyperNodes: 4.62x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from dask import delayed, compute\n",
    "import dask.bag as db\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Assumed existing: func_a, func_b, func_c, HyperNodes pipeline\n",
    "# ---------------------------------------------------------\n",
    "# def func_a(x: float) -> float: ...\n",
    "# def func_b(x: float) -> float: ...\n",
    "# def func_c(x: float) -> float: ...\n",
    "#\n",
    "# @node(output_name=\"a_result\")\n",
    "# def node_a(x: float) -> float:\n",
    "#     return func_a(x)\n",
    "#\n",
    "# @node(output_name=\"b_result\")\n",
    "# def node_b(a_result: float) -> float:\n",
    "#     return func_b(a_result)\n",
    "#\n",
    "# @node(output_name=\"c_result\")\n",
    "# def node_c(b_result: float) -> float:\n",
    "#     return func_c(b_result)\n",
    "#\n",
    "# hpn_pipeline = Pipeline(nodes=[node_a, node_b, node_c])\n",
    "\n",
    "# =========================================================\n",
    "# Benchmark 1: Single Row\n",
    "# =========================================================\n",
    "\n",
    "test_value = 3.0\n",
    "\n",
    "# ---------- HyperNodes (as you had) ----------\n",
    "# Warm-up\n",
    "hpn_pipeline.run(inputs={\"x\": test_value})\n",
    "\n",
    "# Benchmark\n",
    "times_hpn = []\n",
    "for _ in range(10):\n",
    "    start = time.perf_counter()\n",
    "    result_hpn = hpn_pipeline.run(inputs={\"x\": test_value})\n",
    "    times_hpn.append((time.perf_counter() - start) * 1000)\n",
    "\n",
    "avg_hpn = np.mean(times_hpn)\n",
    "std_hpn = np.std(times_hpn)\n",
    "\n",
    "print(\"Single Row: HyperNodes\")\n",
    "print(f\"  Result: {result_hpn}\")\n",
    "print(f\"  Avg: {avg_hpn:.4f}ms (Â±{std_hpn:.4f}ms)\")\n",
    "\n",
    "# ---------- Dask: Single row via delayed DAG ----------\n",
    "\n",
    "\n",
    "@delayed\n",
    "def dask_node_a(x: float) -> float:\n",
    "    return func_a(x)\n",
    "\n",
    "\n",
    "@delayed\n",
    "def dask_node_b(a_result: float) -> float:\n",
    "    return func_b(a_result)\n",
    "\n",
    "\n",
    "@delayed\n",
    "def dask_node_c(b_result: float) -> float:\n",
    "    return func_c(b_result)\n",
    "\n",
    "\n",
    "# Build the Dask DAG once\n",
    "def dask_single_row_graph(x: float):\n",
    "    a = dask_node_a(x)\n",
    "    b = dask_node_b(a)\n",
    "    c = dask_node_c(b)\n",
    "    return c  # delayed object\n",
    "\n",
    "\n",
    "# Warm-up\n",
    "_ = dask_single_row_graph(test_value).compute()\n",
    "\n",
    "# Benchmark\n",
    "times_dask_single = []\n",
    "for _ in range(10):\n",
    "    start = time.perf_counter()\n",
    "    delayed_result = dask_single_row_graph(test_value)\n",
    "    result_dask_single = delayed_result.compute()\n",
    "    times_dask_single.append((time.perf_counter() - start) * 1000)\n",
    "\n",
    "avg_dask_single = np.mean(times_dask_single)\n",
    "std_dask_single = np.std(times_dask_single)\n",
    "\n",
    "print(\"Single Row: Dask (delayed DAG)\")\n",
    "print(f\"  Result: {result_dask_single}\")\n",
    "print(f\"  Avg: {avg_dask_single:.4f}ms (Â±{std_dask_single:.4f}ms)\")\n",
    "print(f\"  vs HyperNodes: {avg_dask_single / avg_hpn:.2f}x\\n\")\n",
    "\n",
    "# =========================================================\n",
    "# Benchmark 2: 100K Rows - Same Functions\n",
    "# =========================================================\n",
    "\n",
    "n_rows = 100_000\n",
    "test_data = np.random.uniform(1, 10, n_rows).tolist()\n",
    "\n",
    "print(f\"Testing with {n_rows:,} rows\")\n",
    "print(f\"Sample input: {test_data[:3]}\\n\")\n",
    "\n",
    "# ---------- HyperNodes: map over 100K rows ----------\n",
    "start = time.perf_counter()\n",
    "hpn_results = hpn_pipeline.map(inputs={\"x\": test_data}, map_over=\"x\")\n",
    "time_hpn_100k = (time.perf_counter() - start) * 1000\n",
    "\n",
    "print(\"100K Rows: HyperNodes\")\n",
    "print(f\"  Time: {time_hpn_100k:.2f}ms\")\n",
    "print(f\"  Per-row: {time_hpn_100k * 1000 / n_rows:.3f}Î¼s\")\n",
    "print(f\"  Sample results: {hpn_results[:2]}\\n\")\n",
    "\n",
    "# ---------- Dask: 100K rows via Bag (row-wise UDFs) ----------\n",
    "\n",
    "# Create a Dask Bag from the list of inputs\n",
    "# You can tune npartitions depending on cores / cluster\n",
    "bag_x = db.from_sequence(test_data, npartitions=32)\n",
    "\n",
    "\n",
    "# Define the pipeline in terms of Dask Bag operations\n",
    "def dask_bag_pipeline(bag):\n",
    "    return bag.map(func_a).map(func_b).map(func_c)\n",
    "\n",
    "\n",
    "# Warm-up (build + compute once)\n",
    "_ = dask_bag_pipeline(bag_x).take(1)\n",
    "\n",
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "bag_results = dask_bag_pipeline(bag_x).compute()\n",
    "time_dask_100k = (time.perf_counter() - start) * 1000\n",
    "\n",
    "print(\"100K Rows: Dask (Bag, row-wise)\")\n",
    "print(f\"  Time: {time_dask_100k:.2f}ms\")\n",
    "print(f\"  Per-row: {time_dask_100k * 1000 / n_rows:.3f}Î¼s\")\n",
    "print(f\"  Sample results: {bag_results[:2]}\")\n",
    "print(f\"  vs HyperNodes: {time_hpn_100k / time_dask_100k:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e76ea79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'daft.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "        .dashboard-container {\n",
       "            display: flex;\n",
       "            gap: 20px;\n",
       "            max-width: 100%;\n",
       "            height: 100%;\n",
       "        }\n",
       "        .table-container {\n",
       "            flex: 1;\n",
       "            overflow: auto;\n",
       "        }\n",
       "        .side-pane {\n",
       "            width: 35%;\n",
       "            max-height: 500px;\n",
       "            border: 1px solid;\n",
       "            border-radius: 4px;\n",
       "            padding: 15px;\n",
       "            display: none;\n",
       "            overflow: auto;\n",
       "        }\n",
       "        .side-pane.visible {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "        }\n",
       "        .side-pane-header {\n",
       "            display: flex;\n",
       "            justify-content: space-between;\n",
       "            align-items: center;\n",
       "            margin-bottom: 10px;\n",
       "            padding-bottom: 10px;\n",
       "            border-bottom: 1px solid;\n",
       "        }\n",
       "        .side-pane-title {\n",
       "            font-weight: bold;\n",
       "        }\n",
       "        .close-button {\n",
       "            cursor: pointer;\n",
       "        }\n",
       "        .side-pane-content {\n",
       "            word-wrap: break-word;\n",
       "            overflow: auto;\n",
       "        }\n",
       "        .dataframe td.clickable {\n",
       "            cursor: pointer;\n",
       "            transition: background-color 0.2s;\n",
       "        }\n",
       "        .dataframe td.clickable:hover {\n",
       "            opacity: 0.8;\n",
       "        }\n",
       "        .dataframe td.clickable.selected {\n",
       "            opacity: 0.6;\n",
       "        }\n",
       "        </style>\n",
       "        <div class=\"dashboard-container\">\n",
       "            <div class=\"table-container\">\n",
       "        <div id=\"dataframe-650ee131-45a5-4388-aa66-3cdab84df232\"><table class=\"dataframe\" style=\"table-layout: fixed; min-width: 100%\">\n",
       "<thead><tr><th style=\"text-wrap: nowrap; width: calc(100vw / 2); min-width: 192px; overflow: hidden; text-overflow: ellipsis; text-align:left\">x<br />Float64</th><th style=\"text-wrap: nowrap; width: calc(100vw / 2); min-width: 192px; overflow: hidden; text-overflow: ellipsis; text-align:left\">a_result<br />Python</th></tr></thead>\n",
       "<tbody>\n",
       "<tr><td data-row=\"0\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">4.370861069626263</div></td><td data-row=\"0\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">6.370861069626263</div></td></tr>\n",
       "<tr><td data-row=\"1\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">9.556428757689245</div></td><td data-row=\"1\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">11.556428757689245</div></td></tr>\n",
       "<tr><td data-row=\"2\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">7.587945476302646</div></td><td data-row=\"2\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">9.587945476302647</div></td></tr>\n",
       "<tr><td data-row=\"3\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">6.387926357773329</div></td><td data-row=\"3\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">8.38792635777333</div></td></tr>\n",
       "<tr><td data-row=\"4\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">2.4041677639819286</div></td><td data-row=\"4\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">4.4041677639819286</div></td></tr>\n",
       "<tr><td data-row=\"5\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">2.403950683025824</div></td><td data-row=\"5\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">4.403950683025824</div></td></tr>\n",
       "<tr><td data-row=\"6\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">1.5227525095137953</div></td><td data-row=\"6\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">3.5227525095137953</div></td></tr>\n",
       "<tr><td data-row=\"7\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">8.795585311974417</div></td><td data-row=\"7\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">10.795585311974417</div></td></tr>\n",
       "</tbody>\n",
       "</table></div>\n",
       "            </div>\n",
       "            <div class=\"side-pane\" id=\"side-pane-650ee131-45a5-4388-aa66-3cdab84df232\">\n",
       "                <div class=\"side-pane-header\">\n",
       "                    <div class=\"side-pane-title\" id=\"side-pane-title-650ee131-45a5-4388-aa66-3cdab84df232\">Cell Details</div>\n",
       "                    <button class=\"close-button\" id=\"close-button-650ee131-45a5-4388-aa66-3cdab84df232\">Ã—</button>\n",
       "                </div>\n",
       "                <div class=\"side-pane-content\" id=\"side-pane-content-650ee131-45a5-4388-aa66-3cdab84df232\">\n",
       "                    <p style=\"font-style: italic;\">Click on a cell to view its full content</p>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "    \n",
       "        <script>\n",
       "        (function() {\n",
       "            const serverUrl = 'http://0.0.0.0:3238';\n",
       "            const dfId = '650ee131-45a5-4388-aa66-3cdab84df232';\n",
       "            const dataframeElement = document.getElementById('dataframe-' + dfId);\n",
       "            const cells = dataframeElement ? dataframeElement.querySelectorAll('td') : [];\n",
       "            const sidePane = document.getElementById('side-pane-' + dfId);\n",
       "            const sidePaneTitle = document.getElementById('side-pane-title-' + dfId);\n",
       "            const sidePaneContent = document.getElementById('side-pane-content-' + dfId);\n",
       "            const closeButton = document.getElementById('close-button-' + dfId);\n",
       "            let selectedCell = null;\n",
       "\n",
       "            function closeSidePane(paneId) {\n",
       "                const pane = document.getElementById('side-pane-' + paneId);\n",
       "                if (pane) {\n",
       "                    pane.classList.remove('visible');\n",
       "                    if (selectedCell) {\n",
       "                        selectedCell.classList.remove('selected');\n",
       "                        selectedCell = null;\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "\n",
       "            function showSidePane(row, col, content) {\n",
       "                sidePaneTitle.textContent = 'Cell (' + row + ', ' + col + ')';\n",
       "                sidePaneContent.innerHTML = content;\n",
       "                sidePane.classList.add('visible');\n",
       "            }\n",
       "\n",
       "            function showLoadingContent() {\n",
       "                sidePaneContent.innerHTML = '<div style=\"text-align:center; padding:20px;\"><span style=\"font-style:italic\">Loading full content...</span></div>';\n",
       "            }\n",
       "\n",
       "            // Add event listener for close button\n",
       "            if (closeButton) {\n",
       "                closeButton.addEventListener('click', function() {\n",
       "                    closeSidePane(dfId);\n",
       "                });\n",
       "            }\n",
       "\n",
       "            cells.forEach((cell) => {\n",
       "                // Skip cells that do not have data-row and data-col attributes (e.g., ellipsis row)\n",
       "                const rowAttr = cell.getAttribute('data-row');\n",
       "                const colAttr = cell.getAttribute('data-col');\n",
       "                if (rowAttr === null || colAttr === null) return;\n",
       "\n",
       "                const row = parseInt(rowAttr);\n",
       "                const col = parseInt(colAttr);\n",
       "                cell.classList.add('clickable');\n",
       "\n",
       "                cell.onclick = function() {\n",
       "                    // Remove selection from previously selected cell\n",
       "                    if (selectedCell && selectedCell !== cell) {\n",
       "                        selectedCell.classList.remove('selected');\n",
       "                    }\n",
       "\n",
       "                    // Toggle selection for current cell\n",
       "                    if (selectedCell === cell) {\n",
       "                        cell.classList.remove('selected');\n",
       "                        selectedCell = null;\n",
       "                        closeSidePane(dfId);\n",
       "                        return;\n",
       "                    } else {\n",
       "                        cell.classList.add('selected');\n",
       "                        selectedCell = cell;\n",
       "                    }\n",
       "\n",
       "                    // Show the side pane immediately\n",
       "                    showSidePane(row, col, '');\n",
       "\n",
       "                    // Set a timeout to show loading content after 1 second\n",
       "                    const loadingTimeout = setTimeout(() => {\n",
       "                        showLoadingContent();\n",
       "                    }, 100);\n",
       "\n",
       "                    // Fetch the cell content\n",
       "                    fetch(serverUrl + '/api/dataframes/' + dfId + '/cell?row=' + row + '&col=' + col)\n",
       "                        .then(response => response.json())\n",
       "                        .then(data => {\n",
       "                            clearTimeout(loadingTimeout);\n",
       "                            showSidePane(row, col, data.value);\n",
       "                        })\n",
       "                        .catch(err => {\n",
       "                            clearTimeout(loadingTimeout);\n",
       "                            // Get the original cell content from the table\n",
       "                            const cell = selectedCell;\n",
       "                            if (cell) {\n",
       "                                const originalContent = cell.innerHTML;\n",
       "                                showSidePane(row, col, originalContent);\n",
       "                            }\n",
       "                        });\n",
       "                };\n",
       "            });\n",
       "        })();\n",
       "        </script>\n",
       "        \n",
       "<small>(Showing first 8 of 100000 rows)</small>"
      ],
      "text/plain": [
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ x                  â”† a_result           â”‚\n",
       "â”‚ ---                â”† ---                â”‚\n",
       "â”‚ Float64            â”† Python             â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 4.370861069626263  â”† 6.370861069626263  â”‚\n",
       "â”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\n",
       "â”‚ 9.556428757689245  â”† 11.556428757689245 â”‚\n",
       "â”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\n",
       "â”‚ 7.587945476302646  â”† 9.587945476302647  â”‚\n",
       "â”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\n",
       "â”‚ 6.387926357773329  â”† 8.38792635777333   â”‚\n",
       "â”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\n",
       "â”‚ 2.4041677639819286 â”† 4.4041677639819286 â”‚\n",
       "â”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\n",
       "â”‚ 2.403950683025824  â”† 4.403950683025824  â”‚\n",
       "â”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\n",
       "â”‚ 1.5227525095137953 â”† 3.5227525095137953 â”‚\n",
       "â”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\n",
       "â”‚ 8.795585311974417  â”† 10.795585311974417 â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "\n",
       "(Showing first 8 of 100000 rows)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Daft: Single row (Row-wise UDFs)\n",
    "@daft.func.batch(return_dtype=DataType.python())\n",
    "def daft_func_a_batch(x, b) -> float:\n",
    "    print(type(x))\n",
    "    return x + b\n",
    "\n",
    "\n",
    "# @daft.func.batch(return_dtype=DataType.python())\n",
    "# def daft_func_b_batch(a_result: float) -> float:\n",
    "#     return func_b(a_result)\n",
    "\n",
    "\n",
    "# @daft.func.batch(return_dtype=DataType.python())\n",
    "# def daft_func_c_batch(b_result: float) -> float:\n",
    "#     return func_c(b_result)\n",
    "\n",
    "\n",
    "# Warm-up\n",
    "(\n",
    "    df_100k.with_column(\"a_result\", daft_func_a_batch(daft.col(\"x\"), daft.lit(2)))\n",
    "    # .with_column(\"b_result\", daft_func_b_batch(daft.col(\"a_result\")))\n",
    "    # .with_column(\"c_result\", daft_func_c_batch(daft.col(\"b_result\")))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "result_daft_batch = (\n",
    "    df_100k.with_column(\"a_result\", daft_func_a_batch(daft.col(\"x\")))\n",
    "    .with_column(\"b_result\", daft_func_b_batch(daft.col(\"a_result\")))\n",
    "    .with_column(\"c_result\", daft_func_c_batch(daft.col(\"b_result\")))\n",
    "    .collect()\n",
    ")\n",
    "time_daft_batch_100k = (time.perf_counter() - start) * 1000\n",
    "\n",
    "results_batch = result_daft_batch.to_pydict()\n",
    "\n",
    "print(\"100K Rows: Daft (Batch UDFs)\")\n",
    "print(f\"  Time: {time_daft_batch_100k:.2f}ms\")\n",
    "print(f\"  Per-row: {time_daft_batch_100k * 1000 / n_rows:.3f}Î¼s\")\n",
    "sample_x_b = results_batch[\"x\"][:2]\n",
    "sample_a_b = results_batch[\"a_result\"][:2]\n",
    "sample_b_b = results_batch[\"b_result\"][:2]\n",
    "sample_c_b = results_batch[\"c_result\"][:2]\n",
    "print(\n",
    "    f\"  Sample (x, a, b, c): {list(zip(sample_x_b, sample_a_b, sample_b_b, sample_c_b))}\"\n",
    ")\n",
    "print(\n",
    "    f\"  vs Daft (Row-wise): {time_daft_rowwise_100k / time_daft_batch_100k:.2f}x faster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270eda9c",
   "metadata": {},
   "source": [
    "## Final Summary: All Results Compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fba099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PERFORMANCE COMPARISON: Same Functions (A â†’ B â†’ C)\n",
      "================================================================================\n",
      " Benchmark        Approach   Time (ms)  Per-Row (Î¼s)\n",
      "Single Row      HyperNodes    0.015858     15.858500\n",
      "Single Row Daft (Row-wise)    1.853808   1853.808201\n",
      " 100K Rows      HyperNodes 1467.816250     14.678162\n",
      " 100K Rows Daft (Row-wise)  590.067041      5.900670\n",
      " 100K Rows    Daft (Batch)    2.846375      0.028464\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š KEY INSIGHTS:\n",
      "\n",
      "1. Single Row:\n",
      "   HyperNodes: 0.0159ms\n",
      "   Daft (Row-wise): 1.8538ms\n",
      "   â†’ Daft has startup overhead on single rows\n",
      "\n",
      "2. 100K Rows (Row-wise):\n",
      "   HyperNodes: 1467.82ms (14.678Î¼s/row)\n",
      "   Daft (Row-wise): 590.07ms (5.901Î¼s/row)\n",
      "   Daft wins by: 2.49x\n",
      "\n",
      "3. 100K Rows (Batch Optimization):\n",
      "   Daft (Batch): 2.85ms (0.028Î¼s/row)\n",
      "   Batch vs Row-wise: 207.30x faster\n",
      "   Batch vs HyperNodes: 515.68x faster\n",
      "\n",
      "âœ… CONCLUSION:\n",
      "   â€¢ Same functions work in all implementations\n",
      "   â€¢ For 100K rows: Daft (batch) is optimal\n",
      "   â€¢ Batch UDFs provide ~100x speedup for vectorizable ops\n"
     ]
    }
   ],
   "source": [
    "# Compile all benchmark results\n",
    "results_summary = {\n",
    "    \"Benchmark\": [\n",
    "        \"Single Row\",\n",
    "        \"Single Row\",\n",
    "        \"100K Rows\",\n",
    "        \"100K Rows\",\n",
    "        \"100K Rows\",\n",
    "    ],\n",
    "    \"Approach\": [\n",
    "        \"HyperNodes\",\n",
    "        \"Daft (Row-wise)\",\n",
    "        \"HyperNodes\",\n",
    "        \"Daft (Row-wise)\",\n",
    "        \"Daft (Batch)\",\n",
    "    ],\n",
    "    \"Time (ms)\": [\n",
    "        avg_hpn,\n",
    "        avg_daft_rowwise,\n",
    "        time_hpn_100k,\n",
    "        time_daft_rowwise_100k,\n",
    "        time_daft_batch_100k,\n",
    "    ],\n",
    "    \"Per-Row (Î¼s)\": [\n",
    "        avg_hpn * 1000,\n",
    "        avg_daft_rowwise * 1000,\n",
    "        time_hpn_100k * 1000 / n_rows,\n",
    "        time_daft_rowwise_100k * 1000 / n_rows,\n",
    "        time_daft_batch_100k * 1000 / n_rows,\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(results_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE COMPARISON: Same Functions (A â†’ B â†’ C)\")\n",
    "print(\"=\" * 80)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“Š KEY INSIGHTS:\")\n",
    "print(f\"\\n1. Single Row:\")\n",
    "print(f\"   HyperNodes: {avg_hpn:.4f}ms\")\n",
    "print(f\"   Daft (Row-wise): {avg_daft_rowwise:.4f}ms\")\n",
    "print(f\"   â†’ Daft has startup overhead on single rows\")\n",
    "\n",
    "print(f\"\\n2. 100K Rows (Row-wise):\")\n",
    "print(\n",
    "    f\"   HyperNodes: {time_hpn_100k:.2f}ms ({time_hpn_100k * 1000 / n_rows:.3f}Î¼s/row)\"\n",
    ")\n",
    "print(\n",
    "    f\"   Daft (Row-wise): {time_daft_rowwise_100k:.2f}ms ({time_daft_rowwise_100k * 1000 / n_rows:.3f}Î¼s/row)\"\n",
    ")\n",
    "print(f\"   Daft wins by: {time_hpn_100k / time_daft_rowwise_100k:.2f}x\")\n",
    "\n",
    "print(f\"\\n3. 100K Rows (Batch Optimization):\")\n",
    "print(\n",
    "    f\"   Daft (Batch): {time_daft_batch_100k:.2f}ms ({time_daft_batch_100k * 1000 / n_rows:.3f}Î¼s/row)\"\n",
    ")\n",
    "print(\n",
    "    f\"   Batch vs Row-wise: {time_daft_rowwise_100k / time_daft_batch_100k:.2f}x faster\"\n",
    ")\n",
    "print(f\"   Batch vs HyperNodes: {time_hpn_100k / time_daft_batch_100k:.2f}x faster\")\n",
    "\n",
    "print(\"\\nâœ… CONCLUSION:\")\n",
    "print(\"   â€¢ Same functions work in all implementations\")\n",
    "print(\"   â€¢ For 100K rows: Daft (batch) is optimal\")\n",
    "print(\"   â€¢ Batch UDFs provide ~100x speedup for vectorizable ops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223736db",
   "metadata": {},
   "source": [
    "## Benchmark 4: Parallel Pipeline (Two Independent Heavy Operations â†’ Consolidation)\n",
    "\n",
    "Testing a diamond-shaped pipeline:\n",
    "- Input splits to two independent heavy computations (A and B)\n",
    "- Results consolidate in node C\n",
    "- This tests parallel execution capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2161cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel pipeline functions test:\n",
      "  heavy_compute_a(0.5) = 1.367260\n",
      "  heavy_compute_b(0.5) = 0.512818\n",
      "  consolidate(0.5, 0.3) = 0.400000\n"
     ]
    }
   ],
   "source": [
    "# Define heavy computation functions\n",
    "def heavy_compute_a(x: float) -> float:\n",
    "    \"\"\"Heavy computation A: Multiple trigonometric operations\"\"\"\n",
    "    result = x\n",
    "    for _ in range(100):\n",
    "        result = np.sin(result) * np.cos(result) + np.sqrt(abs(result))\n",
    "    return result\n",
    "\n",
    "\n",
    "def heavy_compute_b(x: float) -> float:\n",
    "    \"\"\"Heavy computation B: Multiple exponential operations\"\"\"\n",
    "    result = x\n",
    "    for _ in range(100):\n",
    "        result = np.exp(result / 10) / (1 + np.exp(result / 10))\n",
    "    return result\n",
    "\n",
    "\n",
    "def consolidate(a_result: float, b_result: float) -> float:\n",
    "    \"\"\"Consolidate: Combine results from A and B\"\"\"\n",
    "    return (a_result + b_result) / 2\n",
    "\n",
    "\n",
    "# Verify the functions work\n",
    "test_val = 0.5\n",
    "print(\"Parallel pipeline functions test:\")\n",
    "print(f\"  heavy_compute_a({test_val}) = {heavy_compute_a(test_val):.6f}\")\n",
    "print(f\"  heavy_compute_b({test_val}) = {heavy_compute_b(test_val):.6f}\")\n",
    "print(f\"  consolidate(0.5, 0.3) = {consolidate(0.5, 0.3):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b18e59a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing parallel pipeline with 10,000 rows\n",
      "Sample input: [0.43111795784180673, 0.1442343502533306, 0.6036732235535041]\n"
     ]
    }
   ],
   "source": [
    "# Test data for parallel pipeline\n",
    "n_parallel = 10_000  # Using 10K rows for heavy computations\n",
    "test_data_parallel = np.random.uniform(0.1, 0.9, n_parallel).tolist()\n",
    "\n",
    "print(f\"\\nTesting parallel pipeline with {n_parallel:,} rows\")\n",
    "print(f\"Sample input: {test_data_parallel[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86cc673e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperNodes (Sequential, 10,000 rows):\n",
      "  Time: 2888.27ms\n",
      "  Per-row: 288.827Î¼s\n",
      "  Sample results: [{'a_result': np.float64(1.3672602562857559), 'b_result': np.float64(0.5128176319164417), 'consolidated': np.float64(0.9400389441010988)}, {'a_result': np.float64(1.3672602562857554), 'b_result': np.float64(0.5128176319164417), 'consolidated': np.float64(0.9400389441010986)}]\n"
     ]
    }
   ],
   "source": [
    "# HyperNodes: Parallel pipeline with sequential execution\n",
    "\n",
    "\n",
    "@node(output_name=\"a_result\")\n",
    "def node_heavy_a(x: float) -> float:\n",
    "    return heavy_compute_a(x)\n",
    "\n",
    "\n",
    "@node(output_name=\"b_result\")\n",
    "def node_heavy_b(x: float) -> float:\n",
    "    return heavy_compute_b(x)\n",
    "\n",
    "\n",
    "@node(output_name=\"consolidated\")\n",
    "def node_consolidate(a_result: float, b_result: float) -> float:\n",
    "    return consolidate(a_result, b_result)\n",
    "\n",
    "\n",
    "# Sequential execution\n",
    "pipeline_parallel_seq = Pipeline(\n",
    "    nodes=[node_heavy_a, node_heavy_b, node_consolidate],\n",
    "    # engine=HypernodesEngine(node_executor=\"sequential\")\n",
    ")\n",
    "\n",
    "# Warm-up\n",
    "pipeline_parallel_seq.map(inputs={\"x\": test_data_parallel[:100]}, map_over=\"x\")\n",
    "\n",
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "results_hpn_seq = pipeline_parallel_seq.map(\n",
    "    inputs={\"x\": test_data_parallel}, map_over=\"x\"\n",
    ")\n",
    "time_hpn_parallel_seq = (time.perf_counter() - start) * 1000\n",
    "\n",
    "print(f\"HyperNodes (Sequential, {n_parallel:,} rows):\")\n",
    "print(f\"  Time: {time_hpn_parallel_seq:.2f}ms\")\n",
    "print(f\"  Per-row: {time_hpn_parallel_seq * 1000 / n_parallel:.3f}Î¼s\")\n",
    "print(f\"  Sample results: {results_hpn_seq[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4e41daf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HypernodesEngine' from 'hypernodes' (/Users/giladrubin/python_workspace/hypernodes/src/hypernodes/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# HyperNodes: Parallel pipeline with async execution\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhypernodes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HypernodesEngine\n\u001b[32m      4\u001b[39m pipeline_parallel_async = Pipeline(\n\u001b[32m      5\u001b[39m     nodes=[node_heavy_a, node_heavy_b, node_consolidate],\n\u001b[32m      6\u001b[39m     engine=HypernodesEngine(node_executor=\u001b[33m\"\u001b[39m\u001b[33masync\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Warm-up\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'HypernodesEngine' from 'hypernodes' (/Users/giladrubin/python_workspace/hypernodes/src/hypernodes/__init__.py)"
     ]
    }
   ],
   "source": [
    "# HyperNodes: Parallel pipeline with async execution\n",
    "from hypernodes import HypernodesEngine\n",
    "\n",
    "pipeline_parallel_async = Pipeline(\n",
    "    nodes=[node_heavy_a, node_heavy_b, node_consolidate],\n",
    "    engine=HypernodesEngine(node_executor=\"async\"),\n",
    ")\n",
    "\n",
    "# Warm-up\n",
    "pipeline_parallel_async.map(inputs={\"x\": test_data_parallel[:100]}, map_over=\"x\")\n",
    "\n",
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "results_hpn_async = pipeline_parallel_async.map(\n",
    "    inputs={\"x\": test_data_parallel}, map_over=\"x\"\n",
    ")\n",
    "time_hpn_parallel_async = (time.perf_counter() - start) * 1000\n",
    "\n",
    "print(f\"\\nHyperNodes (Async, {n_parallel:,} rows):\")\n",
    "print(f\"  Time: {time_hpn_parallel_async:.2f}ms\")\n",
    "print(f\"  Per-row: {time_hpn_parallel_async * 1000 / n_parallel:.3f}Î¼s\")\n",
    "print(f\"  Sample results: {results_hpn_async[:2]}\")\n",
    "print(\n",
    "    f\"  Speedup vs Sequential: {time_hpn_parallel_seq / time_hpn_parallel_async:.2f}x\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c294e1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Daft (Batch UDFs, 10,000 rows):\n",
      "  Time: 2667.50ms\n",
      "  Per-row: 266.750Î¼s\n",
      "  Sample (x, a, b, consolidated): [(0.43111795784180673, 1.3672602562857559, 0.5128176319164417, 0.9400389441010988), (0.1442343502533306, 1.3672602562857554, 0.5128176319164417, 0.9400389441010986)]\n"
     ]
    }
   ],
   "source": [
    "# Daft: Parallel pipeline with batch UDFs\n",
    "@daft.func.batch(return_dtype=DataType.python())\n",
    "def daft_heavy_a_batch(x_series):\n",
    "    \"\"\"Batch version of heavy_compute_a\"\"\"\n",
    "    x_list = x_series.to_pylist() if hasattr(x_series, \"to_pylist\") else x_series\n",
    "    return [heavy_compute_a(val) for val in x_list]\n",
    "\n",
    "\n",
    "@daft.func.batch(return_dtype=DataType.python())\n",
    "def daft_heavy_b_batch(x_series):\n",
    "    \"\"\"Batch version of heavy_compute_b\"\"\"\n",
    "    x_list = x_series.to_pylist() if hasattr(x_series, \"to_pylist\") else x_series\n",
    "    return [heavy_compute_b(val) for val in x_list]\n",
    "\n",
    "\n",
    "@daft.func.batch(return_dtype=DataType.python())\n",
    "def daft_consolidate_batch(a_series, b_series):\n",
    "    \"\"\"Batch version of consolidate\"\"\"\n",
    "    a_list = a_series.to_pylist() if hasattr(a_series, \"to_pylist\") else a_series\n",
    "    b_list = b_series.to_pylist() if hasattr(b_series, \"to_pylist\") else b_series\n",
    "    return [consolidate(a, b) for a, b in zip(a_list, b_list)]\n",
    "\n",
    "\n",
    "df_parallel = daft.from_pydict({\"x\": test_data_parallel})\n",
    "df_parallel_warmup = daft.from_pydict({\"x\": test_data_parallel[:100]})\n",
    "\n",
    "# Warm-up\n",
    "(\n",
    "    df_parallel_warmup.with_column(\"a_result\", daft_heavy_a_batch(daft.col(\"x\")))\n",
    "    .with_column(\"b_result\", daft_heavy_b_batch(daft.col(\"x\")))\n",
    "    .with_column(\n",
    "        \"consolidated\",\n",
    "        daft_consolidate_batch(daft.col(\"a_result\"), daft.col(\"b_result\")),\n",
    "    )\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Benchmark\n",
    "start = time.perf_counter()\n",
    "result_daft_parallel = (\n",
    "    df_parallel.with_column(\"a_result\", daft_heavy_a_batch(daft.col(\"x\")))\n",
    "    .with_column(\"b_result\", daft_heavy_b_batch(daft.col(\"x\")))\n",
    "    .with_column(\n",
    "        \"consolidated\",\n",
    "        daft_consolidate_batch(daft.col(\"a_result\"), daft.col(\"b_result\")),\n",
    "    )\n",
    "    .collect()\n",
    ")\n",
    "time_daft_parallel = (time.perf_counter() - start) * 1000\n",
    "\n",
    "results_daft = result_daft_parallel.to_pydict()\n",
    "\n",
    "print(f\"\\nDaft (Batch UDFs, {n_parallel:,} rows):\")\n",
    "print(f\"  Time: {time_daft_parallel:.2f}ms\")\n",
    "print(f\"  Per-row: {time_daft_parallel * 1000 / n_parallel:.3f}Î¼s\")\n",
    "sample_results = list(\n",
    "    zip(\n",
    "        results_daft[\"x\"][:2],\n",
    "        results_daft[\"a_result\"][:2],\n",
    "        results_daft[\"b_result\"][:2],\n",
    "        results_daft[\"consolidated\"][:2],\n",
    "    )\n",
    ")\n",
    "print(f\"  Sample (x, a, b, consolidated): {sample_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Pipeline Comparison Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PARALLEL PIPELINE COMPARISON ({n_parallel:,} rows)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Approach':<30} {'Time (ms)':<15} {'Per-row (Î¼s)':<15} {'vs Daft':<10}\")\n",
    "print(\"-\" * 80)\n",
    "print(\n",
    "    f\"{'HyperNodes (Sequential)':<30} {time_hpn_parallel_seq:>10.2f}     {time_hpn_parallel_seq * 1000 / n_parallel:>10.3f}     {time_hpn_parallel_seq / time_daft_parallel:>6.2f}x\"\n",
    ")\n",
    "print(\n",
    "    f\"{'HyperNodes (Async)':<30} {time_hpn_parallel_async:>10.2f}     {time_hpn_parallel_async * 1000 / n_parallel:>10.3f}     {time_hpn_parallel_async / time_daft_parallel:>6.2f}x\"\n",
    ")\n",
    "print(\n",
    "    f\"{'Daft (Batch)':<30} {time_daft_parallel:>10.2f}     {time_daft_parallel * 1000 / n_parallel:>10.3f}     {'1.00x':>10}\"\n",
    ")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“Š KEY INSIGHTS:\")\n",
    "print(f\"\\n1. Async Benefit:\")\n",
    "print(f\"   Sequential: {time_hpn_parallel_seq:.2f}ms\")\n",
    "print(f\"   Async: {time_hpn_parallel_async:.2f}ms\")\n",
    "print(\n",
    "    f\"   â†’ Async provides {time_hpn_parallel_seq / time_hpn_parallel_async:.2f}x speedup for parallel operations\"\n",
    ")\n",
    "\n",
    "print(f\"\\n2. Daft's Efficiency:\")\n",
    "print(f\"   Daft: {time_daft_parallel:.2f}ms\")\n",
    "print(\n",
    "    f\"   â†’ Daft is {time_hpn_parallel_seq / time_daft_parallel:.2f}x faster than HyperNodes Sequential\"\n",
    ")\n",
    "print(\n",
    "    f\"   â†’ Daft is {time_hpn_parallel_async / time_daft_parallel:.2f}x faster than HyperNodes Async\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… CONCLUSION:\")\n",
    "print(\"   â€¢ For parallel heavy computations, Daft's batch processing is optimal\")\n",
    "print(\"   â€¢ HyperNodes async provides benefit over sequential\")\n",
    "print(\"   â€¢ Daft's columnar operations are highly efficient for this workload\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypernodes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

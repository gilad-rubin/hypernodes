{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b640ae",
   "metadata": {},
   "source": [
    "# Translating HyperNodes Pipelines to Daft\n",
    "\n",
    "This notebook demonstrates how to translate HyperNodes pipelines into Daft pipelines for performance gains.\n",
    "\n",
    "We'll progress through increasingly complex examples:\n",
    "1. **Simple transformations**: Basic data processing\n",
    "2. **Map operations**: Processing collections\n",
    "3. **Stateful processing**: Using classes with initialization\n",
    "4. **Complex pipelines**: Multi-stage processing with encoders and indexes\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "**HyperNodes:**\n",
    "- Function-based nodes with explicit inputs/outputs\n",
    "- Sequential execution by default\n",
    "- `.map()` for processing collections\n",
    "- Pipelines compose nodes into DAGs\n",
    "\n",
    "**Daft:**\n",
    "- DataFrame-based operations\n",
    "- Lazy evaluation with automatic optimization\n",
    "- Built-in parallel processing\n",
    "- UDFs for custom logic (`@daft.func`, `@daft.cls`, `@daft.func.batch`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb0a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install daft hypernodes numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066b9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "from typing import Iterator, List\n",
    "\n",
    "import daft\n",
    "import numpy as np\n",
    "from daft import DataType, Series\n",
    "from hypernodes import Pipeline, node\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26de2fb",
   "metadata": {},
   "source": [
    "## Example 1: Simple Text Processing\n",
    "\n",
    "Let's start with basic text transformations: cleaning and tokenizing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c5e399",
   "metadata": {},
   "source": [
    "### HyperNodes Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4472135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperNodes single result: {'cleaned_text': 'hello world', 'tokens': ['hello', 'world'], 'token_count': 2}\n"
     ]
    }
   ],
   "source": [
    "# Define nodes\n",
    "@node(output_name=\"cleaned_text\")\n",
    "def clean_text(text: str) -> str:\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "@node(output_name=\"tokens\")\n",
    "def tokenize(cleaned_text: str) -> List[str]:\n",
    "    return cleaned_text.split()\n",
    "\n",
    "\n",
    "@node(output_name=\"token_count\")\n",
    "def count_tokens(tokens: List[str]) -> int:\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "# Build pipeline\n",
    "text_pipeline_hn = Pipeline(\n",
    "    nodes=[clean_text, tokenize, count_tokens],\n",
    "    name=\"text_processing_hypernodes\",\n",
    ")\n",
    "\n",
    "# Process single text\n",
    "result = text_pipeline_hn.run(inputs={\"text\": \"  Hello World  \"})\n",
    "print(f\"HyperNodes single result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46780fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HyperNodes map results:\n",
      "  '  Hello World  ' -> 2 tokens\n",
      "  '  Daft is FAST  ' -> 3 tokens\n",
      "  '  HyperNodes are modular  ' -> 3 tokens\n",
      "  '  Python for data processing  ' -> 4 tokens\n",
      "Time: 0.0006s\n"
     ]
    }
   ],
   "source": [
    "# Process multiple texts using .map()\n",
    "texts = [\n",
    "    \"  Hello World  \",\n",
    "    \"  Daft is FAST  \",\n",
    "    \"  HyperNodes are modular  \",\n",
    "    \"  Python for data processing  \",\n",
    "]\n",
    "\n",
    "start = time.time()\n",
    "results_hn = text_pipeline_hn.map(inputs={\"text\": texts}, map_over=\"text\")\n",
    "elapsed_hn = time.time() - start\n",
    "\n",
    "print(f\"\\nHyperNodes map results:\")\n",
    "for i, tc in enumerate(results_hn[\"token_count\"]):\n",
    "    print(f\"  '{texts[i]}' -> {tc} tokens\")\n",
    "print(f\"Time: {elapsed_hn:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5509f2f",
   "metadata": {},
   "source": [
    "### Daft Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "743d7a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â±ï¸  Daft Time: 0.0056s\n",
      "ðŸ“Š Speedup: 0.10x\n",
      "\n",
      "Daft results (to_pydict):\n",
      "{'text': ['  Hello World  ', '  Daft is FAST  ', '  HyperNodes are modular  ', '  Python for data processing  '], 'cleaned_text': ['hello world', 'daft is fast', 'hypernodes are modular', 'python for data processing'], 'tokens': [['hello', 'world'], ['daft', 'is', 'fast'], ['hypernodes', 'are', 'modular'], ['python', 'for', 'data', 'processing']], 'token_count': [2, 3, 3, 4]}\n"
     ]
    }
   ],
   "source": [
    "# Define functions using @daft.func\n",
    "@daft.func\n",
    "def clean_text_daft(text: str) -> str:\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "@daft.func\n",
    "def tokenize_daft(text: str) -> list[str]:\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "@daft.func\n",
    "def count_tokens_daft(tokens: list[str]) -> int:\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "# Create DataFrame and apply transformations\n",
    "df_daft = daft.from_pydict({\"text\": texts})\n",
    "\n",
    "start = time.time()\n",
    "df_daft = df_daft.with_column(\"cleaned_text\", clean_text_daft(df_daft[\"text\"]))\n",
    "df_daft = df_daft.with_column(\"tokens\", tokenize_daft(df_daft[\"cleaned_text\"]))\n",
    "df_daft = df_daft.with_column(\"token_count\", count_tokens_daft(df_daft[\"tokens\"]))\n",
    "\n",
    "# Materialize results\n",
    "results_daft = df_daft.collect()\n",
    "elapsed_daft = time.time() - start\n",
    "\n",
    "print(f\"\\nâ±ï¸  Daft Time: {elapsed_daft:.4f}s\")\n",
    "print(f\"ðŸ“Š Speedup: {elapsed_hn / elapsed_daft:.2f}x\")\n",
    "print(\"\\nDaft results (to_pydict):\")\n",
    "print(results_daft.to_pydict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c04a34",
   "metadata": {},
   "source": [
    "**Important limitation**: Daft's built-in string operations are currently limited (`.str.contains()`, `.str.split()`). Common operations like `.strip()` and `.lower()` are NOT available as built-ins. For text cleaning, **UDFs are required**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f7093",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "1. **HyperNodes**: Explicit `map_over` parameter to process collections\n",
    "2. **Daft**: DataFrame operations automatically apply to all rows\n",
    "3. **Daft** uses lazy evaluation - operations are only executed when calling `.collect()` or `.show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd68cbf",
   "metadata": {},
   "source": [
    "### Daft Version - Without UDFs (Built-in Operations)\n",
    "\n",
    "For simple operations, Daft has built-in string and list methods that don't require custom UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a111689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸  Daft built-in time: 0.0020s\n",
      "ðŸ“Š Result: {'text': ['  Hello World  ', '  Daft is FAST  ', '  HyperNodes are modular  ', '  Python for data processing  '], 'token_count': [6, 7, 7, 8]}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Note: Daft doesn't have .str.strip() or .str.lower() built-ins\n",
    "# The string operations available are limited to:\n",
    "#   - .str.contains() - substring search\n",
    "#   - .str.split() - split string into list\n",
    "# For text cleaning, UDFs are necessary\n",
    "\n",
    "df_daft_builtin = daft.from_pydict({\"text\": texts})\n",
    "start = time.time()\n",
    "\n",
    "# We can use split and length without UDFs\n",
    "# This skips the cleaning step (no built-in strip/lower)\n",
    "df_daft_builtin = df_daft_builtin.with_column(\n",
    "    \"tokens\", df_daft_builtin[\"text\"].str.split(\" \")\n",
    ")\n",
    "\n",
    "df_daft_builtin = df_daft_builtin.with_column(\n",
    "    \"token_count\", df_daft_builtin[\"tokens\"].list.length()\n",
    ")\n",
    "\n",
    "result_builtin = df_daft_builtin.select(\"text\", \"token_count\").collect()\n",
    "elapsed_builtin = time.time() - start\n",
    "\n",
    "print(f\"â±ï¸  Daft built-in time: {elapsed_builtin:.4f}s\")\n",
    "print(f\"ðŸ“Š Result: {result_builtin.to_pydict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad6576",
   "metadata": {},
   "source": [
    "**Key insight**: Built-in operations are often faster than UDFs! Daft has optimized implementations for common operations like:\n",
    "- `.str.strip()`, `.str.lower()`, `.str.split()` for strings\n",
    "- `.list.length()`, `.list.get()` for lists\n",
    "- Arithmetic operations, comparisons\n",
    "- Use UDFs only when you need custom logic not available as built-ins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c55a3ae",
   "metadata": {},
   "source": [
    "## Example 2: Generator Functions - Text Tokenization\n",
    "\n",
    "Let's process text where each input produces multiple outputs (one row per token)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8034fe50",
   "metadata": {},
   "source": [
    "### HyperNodes Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5e8397b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperNodes tokens: ['hello', 'world', 'daft', 'is', 'fast', 'python', 'rocks']\n"
     ]
    }
   ],
   "source": [
    "# In HyperNodes, we return a list and then flatten\n",
    "@node(output_name=\"tokens\")\n",
    "def tokenize_to_list(text: str) -> List[str]:\n",
    "    return text.strip().lower().split()\n",
    "\n",
    "\n",
    "@node(output_name=\"token\")\n",
    "def flatten_tokens(tokens: List[str]) -> List[str]:\n",
    "    # This would need to be handled specially in HyperNodes\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Simple approach: process manually\n",
    "sentences = [\"Hello World\", \"Daft is fast\", \"Python rocks\"]\n",
    "\n",
    "all_tokens_hn = []\n",
    "for sent in sentences:\n",
    "    result = tokenize_to_list.func(sent)\n",
    "    all_tokens_hn.extend(result)\n",
    "\n",
    "print(f\"HyperNodes tokens: {all_tokens_hn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d1d27",
   "metadata": {},
   "source": [
    "### Daft Version with Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d745584d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Daft generator results (to_pydict):\n",
      "{'sentence': ['Hello World', 'Hello World', 'Daft is fast', 'Daft is fast', 'Daft is fast', 'Python rocks', 'Python rocks'], 'token': ['hello', 'world', 'daft', 'is', 'fast', 'python', 'rocks']}\n"
     ]
    }
   ],
   "source": [
    "@daft.func\n",
    "def tokenize_generator(text: str) -> Iterator[str]:\n",
    "    \"\"\"Generator that yields one token at a time.\"\"\"\n",
    "    for token in text.strip().lower().split():\n",
    "        yield token\n",
    "\n",
    "\n",
    "df_gen = daft.from_pydict({\"sentence\": sentences})\n",
    "df_gen = df_gen.select(\n",
    "    \"sentence\", tokenize_generator(df_gen[\"sentence\"]).alias(\"token\")\n",
    ")\n",
    "df_gen = df_gen.collect()\n",
    "\n",
    "print(\"\\nDaft generator results (to_pydict):\")\n",
    "print(df_gen.to_pydict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0e2a3",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "1. **Daft generators** automatically expand rows - the `sentence` column is broadcast\n",
    "2. **HyperNodes** requires manual flattening or special handling\n",
    "3. **Daft's approach** is more declarative and handles the complexity internally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923fa427",
   "metadata": {},
   "source": [
    "### Daft Version - Without UDFs (Built-in Explode)\n",
    "\n",
    "Daft has a built-in `.explode()` method for expanding lists into multiple rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be744a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Daft built-in explode results (to_pydict):\n",
      "{'sentence': ['Hello World', 'Hello World', 'Daft is fast', 'Daft is fast', 'Daft is fast', 'Python rocks', 'Python rocks'], 'token': ['Hello', 'World', 'Daft', 'is', 'fast', 'Python', 'rocks']}\n"
     ]
    }
   ],
   "source": [
    "df_gen_builtin = daft.from_pydict({\"sentence\": sentences})\n",
    "\n",
    "# Note: We can only use .str.split() - no .strip() or .lower() built-ins\n",
    "# So results won't match exactly (won't be lowercased)\n",
    "df_gen_builtin = df_gen_builtin.with_column(\n",
    "    \"tokens\", df_gen_builtin[\"sentence\"].str.split(\" \")\n",
    ")\n",
    "\n",
    "# Use .explode() to expand the list into multiple rows\n",
    "df_gen_builtin = df_gen_builtin.explode(\"tokens\")\n",
    "\n",
    "# Rename for clarity\n",
    "df_gen_builtin = df_gen_builtin.select(\n",
    "    \"sentence\", df_gen_builtin[\"tokens\"].alias(\"token\")\n",
    ")\n",
    "df_gen_builtin = df_gen_builtin.collect()\n",
    "\n",
    "print(\"\\nDaft built-in explode results (to_pydict):\")\n",
    "print(df_gen_builtin.to_pydict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf241f",
   "metadata": {},
   "source": [
    "**Key insight**: `.explode()` is the built-in alternative to generator UDFs when you already have a list column. It's more efficient than a generator UDF for this use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20015024",
   "metadata": {},
   "source": [
    "## Example 3: Stateful Processing with Classes\n",
    "\n",
    "Now let's use a class with expensive initialization (simulating model loading)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d6bf5",
   "metadata": {},
   "source": [
    "### HyperNodes Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a1bc2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [HN] Initializing encoder with dim=8, seed=42\n",
      "\n",
      "HyperNodes encoding: 4 embeddings\n",
      "Time: 0.0009s\n",
      "Sample embedding shape: (8,)\n",
      "\n",
      "HyperNodes encoding: 4 embeddings\n",
      "Time: 0.0009s\n",
      "Sample embedding shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "class SimpleEncoder:\n",
    "    \"\"\"Simulates an encoder with expensive initialization.\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, seed: int = 42):\n",
    "        print(f\"  [HN] Initializing encoder with dim={dim}, seed={seed}\")\n",
    "        time.sleep(0.5)  # Simulate expensive initialization\n",
    "        self.dim = dim\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def encode(self, text: str) -> np.ndarray:\n",
    "        # Simulate encoding\n",
    "        return self.rng.random(self.dim, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Create encoder instance (expensive!)\n",
    "encoder_hn = SimpleEncoder(dim=8)\n",
    "\n",
    "\n",
    "@node(output_name=\"embedding\")\n",
    "def encode_text_hn(text: str, encoder: SimpleEncoder) -> np.ndarray:\n",
    "    return encoder.encode(text)\n",
    "\n",
    "\n",
    "# Create pipeline\n",
    "encode_pipeline_hn = Pipeline(nodes=[encode_text_hn], name=\"encode_hn\")\n",
    "\n",
    "# Process multiple texts\n",
    "texts_encode = [\"hello\", \"world\", \"daft\", \"hypernodes\"]\n",
    "\n",
    "start = time.time()\n",
    "results_encode_hn = encode_pipeline_hn.map(\n",
    "    inputs={\"text\": texts_encode, \"encoder\": encoder_hn}, map_over=\"text\"\n",
    ")\n",
    "elapsed_encode_hn = time.time() - start\n",
    "\n",
    "print(f\"\\nHyperNodes encoding: {len(results_encode_hn['embedding'])} embeddings\")\n",
    "print(f\"Time: {elapsed_encode_hn:.4f}s\")\n",
    "print(f\"Sample embedding shape: {results_encode_hn['embedding'][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a997a4",
   "metadata": {},
   "source": [
    "### Daft Version with @daft.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "670fa325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Daft] Initializing encoder with dim=8, seed=42\n",
      "\n",
      "â±ï¸  Daft encoding: 4 embeddings\n",
      "â±ï¸  Time: 0.5121s\n",
      "Sample embedding shape: (8,)\n",
      "\n",
      "â±ï¸  Daft encoding: 4 embeddings\n",
      "â±ï¸  Time: 0.5121s\n",
      "Sample embedding shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "@daft.cls\n",
    "class SimpleEncoderDaft:\n",
    "    \"\"\"Daft encoder - initialization happens once per worker.\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, seed: int = 42):\n",
    "        print(f\"  [Daft] Initializing encoder with dim={dim}, seed={seed}\")\n",
    "        time.sleep(0.5)  # Simulate expensive initialization\n",
    "        self.dim = dim\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    @daft.method(return_dtype=DataType.python())\n",
    "    def encode(self, text: str) -> np.ndarray:\n",
    "        return self.rng.random(self.dim, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Create encoder instance (lazy - doesn't initialize yet!)\n",
    "encoder_daft = SimpleEncoderDaft(dim=8)\n",
    "\n",
    "df_encode = daft.from_pydict({\"text\": texts_encode})\n",
    "\n",
    "start = time.time()\n",
    "df_encode = df_encode.with_column(\"embedding\", encoder_daft.encode(df_encode[\"text\"]))\n",
    "results_encode_daft = df_encode.collect()\n",
    "elapsed_encode_daft = time.time() - start\n",
    "\n",
    "print(f\"\\nâ±ï¸  Daft encoding: {results_encode_daft.count_rows()} embeddings\")\n",
    "print(f\"â±ï¸  Time: {elapsed_encode_daft:.4f}s\")\n",
    "print(\n",
    "    f\"Sample embedding shape: {results_encode_daft.to_pydict()['embedding'][0].shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608eccc",
   "metadata": {},
   "source": [
    "### Daft Version - Without UDFs?\n",
    "\n",
    "**Note**: This example demonstrates stateful processing with expensive initialization (simulating loading a model). Daft's built-in operations cannot handle this use case - **UDFs are required** when you need:\n",
    "- Expensive initialization that should happen once per worker\n",
    "- Stateful processing with persistent state across rows\n",
    "- Custom logic that goes beyond simple transformations\n",
    "\n",
    "For this scenario, the `@daft.cls` UDF approach shown above is the correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939522fd",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "1. **HyperNodes**: Encoder initialized once upfront, passed to each invocation\n",
    "2. **Daft**: Encoder initialized lazily per worker during execution\n",
    "3. **Daft's lazy init** is powerful for distributed execution where the encoder can't be serialized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682ec7f",
   "metadata": {},
   "source": [
    "## Example 4: Batch Processing with NumPy\n",
    "\n",
    "Let's leverage vectorized operations for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582a6eb4",
   "metadata": {},
   "source": [
    "### HyperNodes Version (Row-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "947aabe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperNodes normalization: 1000 values\n",
      "Time: 0.3038s\n",
      "Sample: [np.float64(-5.0), np.float64(-4.98998998998999), np.float64(-4.97997997997998)]\n"
     ]
    }
   ],
   "source": [
    "@node(output_name=\"normalized\")\n",
    "def normalize_value_hn(value: float, mean: float, std: float) -> float:\n",
    "    return (value - mean) / std\n",
    "\n",
    "\n",
    "# Create pipeline\n",
    "norm_pipeline_hn = Pipeline(nodes=[normalize_value_hn], name=\"normalize_hn\")\n",
    "\n",
    "# Sample data\n",
    "values = list(np.linspace(0, 100, 1000))\n",
    "mean_val = 50.0\n",
    "std_val = 10.0\n",
    "\n",
    "start = time.time()\n",
    "results_norm_hn = norm_pipeline_hn.map(\n",
    "    inputs={\"value\": values, \"mean\": mean_val, \"std\": std_val}, map_over=\"value\"\n",
    ")\n",
    "elapsed_norm_hn = time.time() - start\n",
    "\n",
    "print(f\"HyperNodes normalization: {len(results_norm_hn['normalized'])} values\")\n",
    "print(f\"Time: {elapsed_norm_hn:.4f}s\")\n",
    "print(f\"Sample: {results_norm_hn['normalized'][:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d89ed47",
   "metadata": {},
   "source": [
    "### Daft Version with Batch UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d80bd6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â±ï¸  Daft batch normalization: 1000 values\n",
      "â±ï¸  Time: 0.1328s\n",
      "ðŸ“Š Speedup: 2.29x\n",
      "Sample (first 3): [-5.0, -4.98998998998999, -4.97997997997998]\n"
     ]
    }
   ],
   "source": [
    "@daft.func.batch(return_dtype=DataType.float64())\n",
    "def normalize_batch(values: Series, mean: float, std: float) -> Series:\n",
    "    \"\"\"Vectorized normalization using NumPy.\"\"\"\n",
    "    arr = values.to_arrow().to_numpy()\n",
    "    normalized = (arr - mean) / std\n",
    "    return Series.from_numpy(normalized)\n",
    "\n",
    "\n",
    "df_norm = daft.from_pydict({\"value\": values})\n",
    "\n",
    "start = time.time()\n",
    "df_norm = df_norm.with_column(\n",
    "    \"normalized\", normalize_batch(df_norm[\"value\"], mean_val, std_val)\n",
    ")\n",
    "results_norm_daft = df_norm.collect()\n",
    "elapsed_norm_daft = time.time() - start\n",
    "\n",
    "print(f\"\\nâ±ï¸  Daft batch normalization: {results_norm_daft.count_rows()} values\")\n",
    "print(f\"â±ï¸  Time: {elapsed_norm_daft:.4f}s\")\n",
    "print(f\"ðŸ“Š Speedup: {elapsed_norm_hn / elapsed_norm_daft:.2f}x\")\n",
    "print(f\"Sample (first 3): {results_norm_daft.to_pydict()['normalized'][:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf72c42",
   "metadata": {},
   "source": [
    "### Daft Version - Without UDFs (Batch Built-in)\n",
    "\n",
    "For simple batch operations on NumPy arrays, we can sometimes use Daft's arithmetic operations instead of batch UDFs. However, this example shows a case where `.batch()` is still beneficial for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccea2266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸  Daft built-in time: 0.0035s\n",
      "ðŸ“Š Result: {'value': [1.0, 2.0, 3.0, 4.0, 5.0], 'normalized': [-1.414213562373095, -0.7071067811865475, 0.0, 0.7071067811865475, 1.414213562373095]}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Using element-wise operations instead of batch UDF\n",
    "values = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "\n",
    "start = time.perf_counter()\n",
    "df_batch_builtin = daft.from_pydict({\"value\": values})\n",
    "\n",
    "# Daft can perform vectorized operations across the column\n",
    "# The operations are applied element-wise but optimized internally\n",
    "mean_val = 3.0  # We'll hardcode mean for simplicity\n",
    "std_val = np.std(values)\n",
    "\n",
    "df_batch_builtin = df_batch_builtin.with_column(\n",
    "    \"normalized\", (df_batch_builtin[\"value\"] - mean_val) / std_val\n",
    ")\n",
    "\n",
    "result_batch_builtin = df_batch_builtin.collect()\n",
    "elapsed_builtin = time.perf_counter() - start\n",
    "\n",
    "print(f\"â±ï¸  Daft built-in time: {elapsed_builtin:.4f}s\")\n",
    "print(f\"ðŸ“Š Result: {result_batch_builtin.to_pydict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df449363",
   "metadata": {},
   "source": [
    "**Trade-off**: Element-wise operations work but may be less efficient than batch UDFs for operations that benefit from vectorization. However, for simple arithmetic, built-in operations are cleaner and often sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbea068",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "1. **Batch processing** can be significantly faster for vectorizable operations\n",
    "2. **Daft's `@daft.func.batch`** makes it easy to leverage NumPy/PyArrow\n",
    "3. **HyperNodes** processes row-by-row by default (though you could manually batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b110b",
   "metadata": {},
   "source": [
    "## Example 5: Complex Pipeline - Document Encoding\n",
    "\n",
    "Let's build a more realistic pipeline similar to the retrieval notebook:\n",
    "1. Load documents\n",
    "2. Clean text\n",
    "3. Encode with a model\n",
    "4. Build an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "346a5c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/jv_rv_890db49y6c1pkmm2l00000gn/T/ipykernel_9199/731529403.py:7: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  class EncodedDocument(BaseModel):\n"
     ]
    }
   ],
   "source": [
    "# Data models\n",
    "class Document(BaseModel):\n",
    "    doc_id: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "class EncodedDocument(BaseModel):\n",
    "    doc_id: str\n",
    "    text: str\n",
    "    embedding: np.ndarray\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    Document(doc_id=\"d1\", text=\"  Machine learning is amazing  \"),\n",
    "    Document(doc_id=\"d2\", text=\"  Python is great for data science  \"),\n",
    "    Document(doc_id=\"d3\", text=\"  Daft provides fast dataframes  \"),\n",
    "    Document(doc_id=\"d4\", text=\"  HyperNodes enables modular pipelines  \"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0d0f4",
   "metadata": {},
   "source": [
    "### HyperNodes Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f53bd7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [HN] Initializing encoder with dim=16, seed=42\n",
      "HyperNodes document pipeline: 4 documents\n",
      "Time: 0.0009s\n",
      "  d1: machine learning is amazing... -> embedding shape (16,)\n",
      "  d2: python is great for data scien... -> embedding shape (16,)\n",
      "HyperNodes document pipeline: 4 documents\n",
      "Time: 0.0009s\n",
      "  d1: machine learning is amazing... -> embedding shape (16,)\n",
      "  d2: python is great for data scien... -> embedding shape (16,)\n"
     ]
    }
   ],
   "source": [
    "@node(output_name=\"cleaned_doc\")\n",
    "def clean_document_hn(doc: Document) -> Document:\n",
    "    return Document(doc_id=doc.doc_id, text=doc.text.strip().lower())\n",
    "\n",
    "\n",
    "@node(output_name=\"encoded_doc\")\n",
    "def encode_document_hn(\n",
    "    cleaned_doc: Document, encoder: SimpleEncoder\n",
    ") -> EncodedDocument:\n",
    "    embedding = encoder.encode(cleaned_doc.text)\n",
    "    return EncodedDocument(\n",
    "        doc_id=cleaned_doc.doc_id, text=cleaned_doc.text, embedding=embedding\n",
    "    )\n",
    "\n",
    "\n",
    "# Build pipeline\n",
    "doc_pipeline_hn = Pipeline(\n",
    "    nodes=[clean_document_hn, encode_document_hn],\n",
    "    name=\"document_encoding_hypernodes\",\n",
    ")\n",
    "\n",
    "# Process documents\n",
    "encoder_doc_hn = SimpleEncoder(dim=16, seed=42)\n",
    "\n",
    "start = time.time()\n",
    "results_doc_hn = doc_pipeline_hn.map(\n",
    "    inputs={\"doc\": documents, \"encoder\": encoder_doc_hn}, map_over=\"doc\"\n",
    ")\n",
    "elapsed_doc_hn = time.time() - start\n",
    "\n",
    "print(f\"HyperNodes document pipeline: {len(results_doc_hn['encoded_doc'])} documents\")\n",
    "print(f\"Time: {elapsed_doc_hn:.4f}s\")\n",
    "for enc_doc in results_doc_hn[\"encoded_doc\"][:2]:\n",
    "    print(\n",
    "        f\"  {enc_doc.doc_id}: {enc_doc.text[:30]}... -> embedding shape {enc_doc.embedding.shape}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e30c54",
   "metadata": {},
   "source": [
    "### Daft Version - Without UDFs?\n",
    "\n",
    "**Note**: This complex pipeline combines:\n",
    "1. Text processing (could use built-ins: `.str.strip()`, `.str.lower()`)\n",
    "2. Custom encoding logic (requires UDF - no built-in alternative)\n",
    "3. Stateful processing with expensive initialization (requires `@daft.cls`)\n",
    "\n",
    "**Conclusion**: Complex real-world pipelines typically require a mix of built-in operations and UDFs. Use built-ins where possible for performance, but don't avoid UDFs when you need custom logic or stateful processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4281ab86",
   "metadata": {},
   "source": [
    "### Daft Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63ad7899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Daft] Initializing DocumentEncoder dim=16\n",
      "\n",
      "â±ï¸  Daft document pipeline: 4 documents\n",
      "â±ï¸  Time: 0.5084s\n",
      "ðŸ“Š Speedup: 0.00x\n",
      "\n",
      "Sample results (first 2):\n",
      "  d1: machine learning is amazing... -> shape (16,)\n",
      "  d2: python is great for data scien... -> shape (16,)\n",
      "\n",
      "â±ï¸  Daft document pipeline: 4 documents\n",
      "â±ï¸  Time: 0.5084s\n",
      "ðŸ“Š Speedup: 0.00x\n",
      "\n",
      "Sample results (first 2):\n",
      "  d1: machine learning is amazing... -> shape (16,)\n",
      "  d2: python is great for data scien... -> shape (16,)\n"
     ]
    }
   ],
   "source": [
    "@daft.func\n",
    "def clean_text_simple(text: str) -> str:\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "@daft.cls\n",
    "class DocumentEncoder:\n",
    "    def __init__(self, dim: int, seed: int = 42):\n",
    "        print(f\"  [Daft] Initializing DocumentEncoder dim={dim}\")\n",
    "        time.sleep(0.5)\n",
    "        self.dim = dim\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    @daft.method(return_dtype=DataType.python())\n",
    "    def encode(self, text: str) -> np.ndarray:\n",
    "        return self.rng.random(self.dim, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Create DataFrame from documents\n",
    "df_docs = daft.from_pydict(\n",
    "    {\"doc_id\": [d.doc_id for d in documents], \"text\": [d.text for d in documents]}\n",
    ")\n",
    "\n",
    "# Create encoder instance\n",
    "encoder_daft_doc = DocumentEncoder(dim=16, seed=42)\n",
    "\n",
    "start = time.time()\n",
    "df_docs = df_docs.with_column(\"cleaned_text\", clean_text_simple(df_docs[\"text\"]))\n",
    "df_docs = df_docs.with_column(\n",
    "    \"embedding\", encoder_daft_doc.encode(df_docs[\"cleaned_text\"])\n",
    ")\n",
    "df_docs = df_docs.select(\"doc_id\", \"cleaned_text\", \"embedding\")\n",
    "\n",
    "results_doc_daft = df_docs.collect()\n",
    "elapsed_doc_daft = time.time() - start\n",
    "\n",
    "print(f\"\\nâ±ï¸  Daft document pipeline: {results_doc_daft.count_rows()} documents\")\n",
    "print(f\"â±ï¸  Time: {elapsed_doc_daft:.4f}s\")\n",
    "print(f\"ðŸ“Š Speedup: {elapsed_doc_hn / elapsed_doc_daft:.2f}x\")\n",
    "print(\"\\nSample results (first 2):\")\n",
    "result_dict = results_doc_daft.to_pydict()\n",
    "for i in range(min(2, len(result_dict[\"doc_id\"]))):\n",
    "    print(\n",
    "        f\"  {result_dict['doc_id'][i]}: {result_dict['cleaned_text'][i][:30]}... -> shape {result_dict['embedding'][i].shape}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6da79",
   "metadata": {},
   "source": [
    "## Example 6: Nested Structure Handling\n",
    "\n",
    "Let's work with the struct unnesting feature from Daft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b2e4e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Daft struct unnesting (to_pydict):\n",
      "{'text': ['hello world', 'daft is fast', 'python for data'], 'word_count': [2, 3, 3], 'char_count': [11, 12, 15]}\n"
     ]
    }
   ],
   "source": [
    "@daft.func(\n",
    "    return_dtype=DataType.struct(\n",
    "        {\"word_count\": DataType.int64(), \"char_count\": DataType.int64()}\n",
    "    ),\n",
    "    unnest=True,\n",
    ")\n",
    "def analyze_text(text: str) -> dict:\n",
    "    words = text.split()\n",
    "    return {\"word_count\": len(words), \"char_count\": len(text)}\n",
    "\n",
    "\n",
    "df_analyze = daft.from_pydict(\n",
    "    {\"text\": [\"hello world\", \"daft is fast\", \"python for data\"]}\n",
    ")\n",
    "\n",
    "df_analyze = df_analyze.select(\"text\", analyze_text(df_analyze[\"text\"]))\n",
    "df_analyze = df_analyze.collect()\n",
    "\n",
    "print(\"\\nDaft struct unnesting (to_pydict):\")\n",
    "print(df_analyze.to_pydict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b362676",
   "metadata": {},
   "source": [
    "### HyperNodes Equivalent\n",
    "\n",
    "In HyperNodes, you would need separate nodes for each field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1498fdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HyperNodes analysis:\n",
      "  'hello world' -> words: 2, chars: 11\n",
      "  'daft is fast' -> words: 3, chars: 12\n",
      "  'python for data' -> words: 3, chars: 15\n"
     ]
    }
   ],
   "source": [
    "@node(output_name=\"word_count\")\n",
    "def count_words_hn(text: str) -> int:\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "@node(output_name=\"char_count\")\n",
    "def count_chars_hn(text: str) -> int:\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "analyze_pipeline_hn = Pipeline(\n",
    "    nodes=[count_words_hn, count_chars_hn], name=\"analyze_hypernodes\"\n",
    ")\n",
    "\n",
    "texts_analyze = [\"hello world\", \"daft is fast\", \"python for data\"]\n",
    "results_analyze_hn = analyze_pipeline_hn.map(\n",
    "    inputs={\"text\": texts_analyze}, map_over=\"text\"\n",
    ")\n",
    "\n",
    "print(\"\\nHyperNodes analysis:\")\n",
    "for i, text in enumerate(texts_analyze):\n",
    "    print(\n",
    "        f\"  '{text}' -> words: {results_analyze_hn['word_count'][i]}, chars: {results_analyze_hn['char_count'][i]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd9bfea",
   "metadata": {},
   "source": [
    "### Daft Version - Without UDFs (Struct Built-ins)\n",
    "\n",
    "Daft has excellent built-in support for struct operations. You can access struct fields directly without UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "604e6e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸  Daft built-in time: 0.0038s\n",
      "ðŸ“Š Result: {'id': [1, 2], 'text': ['hello world', 'bonjour monde'], 'lang': ['en', 'fr'], 'word_count': [2, 2]}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Using struct field access instead of UDFs\n",
    "# Correct format for from_pydict with nested structs\n",
    "docs = {\n",
    "    \"doc\": [\n",
    "        {\"id\": 1, \"text\": \"hello world\", \"meta\": {\"lang\": \"en\"}},\n",
    "        {\"id\": 2, \"text\": \"bonjour monde\", \"meta\": {\"lang\": \"fr\"}},\n",
    "    ]\n",
    "}\n",
    "\n",
    "start = time.perf_counter()\n",
    "df_struct_builtin = daft.from_pydict(docs)\n",
    "\n",
    "# Access struct fields directly using dot notation\n",
    "df_struct_builtin = df_struct_builtin.with_column(\"id\", df_struct_builtin[\"doc\"][\"id\"])\n",
    "df_struct_builtin = df_struct_builtin.with_column(\n",
    "    \"text\", df_struct_builtin[\"doc\"][\"text\"]\n",
    ")\n",
    "df_struct_builtin = df_struct_builtin.with_column(\n",
    "    \"lang\", df_struct_builtin[\"doc\"][\"meta\"][\"lang\"]\n",
    ")\n",
    "\n",
    "# Calculate word count using built-in string operations\n",
    "df_struct_builtin = df_struct_builtin.with_column(\n",
    "    \"word_count\", df_struct_builtin[\"text\"].str.split(\" \").list.length()\n",
    ")\n",
    "\n",
    "# Select relevant columns\n",
    "df_struct_builtin = df_struct_builtin.select(\"id\", \"text\", \"lang\", \"word_count\")\n",
    "\n",
    "result_struct_builtin = df_struct_builtin.collect()\n",
    "elapsed_builtin = time.perf_counter() - start\n",
    "\n",
    "print(f\"â±ï¸  Daft built-in time: {elapsed_builtin:.4f}s\")\n",
    "print(f\"ðŸ“Š Result: {result_struct_builtin.to_pydict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743a56e",
   "metadata": {},
   "source": [
    "**Winner for structs**: Built-in struct field access is cleaner and more efficient than UDFs. Use `df[\"struct_col\"][\"field\"]` to access nested fields directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7755a2",
   "metadata": {},
   "source": [
    "## Example 7: Heavy Performance Test - Document Processing Pipeline\n",
    "\n",
    "Let's create a realistic, compute-intensive pipeline inspired by the retrieval notebook:\n",
    "1. Generate 1000 synthetic documents\n",
    "2. Clean and tokenize text\n",
    "3. Compute TF-IDF-like scores (batch operations)\n",
    "4. Generate embeddings with simulated model\n",
    "5. Aggregate statistics\n",
    "\n",
    "This tests real-world performance with significant data volume and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2842b6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5000 synthetic documents\n",
      "Sample: {'doc_id': 'doc_0000', 'text': 'classification matrix train network neural regression train train science network data learning test algorithm learning machine test test gradient model optimization matrix matrix regression test classification data python regression neural deep neural descent science vector descent deep learning classification matrix neural test network matrix data vector gradient science'}\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic documents\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = [\n",
    "    \"machine\",\n",
    "    \"learning\",\n",
    "    \"data\",\n",
    "    \"science\",\n",
    "    \"python\",\n",
    "    \"algorithm\",\n",
    "    \"neural\",\n",
    "    \"network\",\n",
    "    \"deep\",\n",
    "    \"model\",\n",
    "    \"train\",\n",
    "    \"test\",\n",
    "    \"feature\",\n",
    "    \"vector\",\n",
    "    \"matrix\",\n",
    "    \"optimization\",\n",
    "    \"gradient\",\n",
    "    \"descent\",\n",
    "    \"regression\",\n",
    "    \"classification\",\n",
    "]\n",
    "\n",
    "# Increase to 5000 documents for better performance testing\n",
    "num_docs = 5000\n",
    "doc_length_range = (10, 50)\n",
    "\n",
    "synthetic_docs = []\n",
    "for i in range(num_docs):\n",
    "    length = np.random.randint(*doc_length_range)\n",
    "    words = np.random.choice(vocab, size=length, replace=True)\n",
    "    text = \" \".join(words)\n",
    "    synthetic_docs.append({\"doc_id\": f\"doc_{i:04d}\", \"text\": text})\n",
    "\n",
    "print(f\"Generated {len(synthetic_docs)} synthetic documents\")\n",
    "print(f\"Sample: {synthetic_docs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0954c6",
   "metadata": {},
   "source": [
    "### HyperNodes Version - Heavy Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e81770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [HN] Initializing encoder with dim=128, seed=42\n",
      "Running HyperNodes heavy pipeline...\n",
      "Running HyperNodes heavy pipeline...\n",
      "\n",
      "â±ï¸  HyperNodes Heavy Pipeline:\n",
      "   Processed: 5000 documents\n",
      "   Time: 2.0589s\n",
      "   Throughput: 2428.46 docs/sec\n",
      "   Avg doc length: 29.5 tokens\n",
      "   Avg unique terms: 14.8\n",
      "\n",
      "â±ï¸  HyperNodes Heavy Pipeline:\n",
      "   Processed: 5000 documents\n",
      "   Time: 2.0589s\n",
      "   Throughput: 2428.46 docs/sec\n",
      "   Avg doc length: 29.5 tokens\n",
      "   Avg unique terms: 14.8\n"
     ]
    }
   ],
   "source": [
    "# Define processing nodes\n",
    "@node(output_name=\"cleaned\")\n",
    "def clean_doc_text(text: str) -> str:\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "@node(output_name=\"tokens\")\n",
    "def tokenize_doc(cleaned: str) -> List[str]:\n",
    "    return cleaned.split()\n",
    "\n",
    "\n",
    "@node(output_name=\"term_freq\")\n",
    "def compute_term_freq(tokens: List[str]) -> dict:\n",
    "    \"\"\"Compute term frequency.\"\"\"\n",
    "    freq = {}\n",
    "    for token in tokens:\n",
    "        freq[token] = freq.get(token, 0) + 1\n",
    "    return freq\n",
    "\n",
    "\n",
    "@node(output_name=\"embedding\")\n",
    "def encode_doc_heavy(text: str, encoder: SimpleEncoder) -> np.ndarray:\n",
    "    \"\"\"Encode document with some computational overhead.\"\"\"\n",
    "    # Simulate more expensive encoding\n",
    "    embedding = encoder.encode(text)\n",
    "    # Add some computation\n",
    "    embedding = embedding * np.sqrt(np.sum(embedding**2) + 1e-8)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "@node(output_name=\"doc_length\")\n",
    "def compute_doc_length(tokens: List[str]) -> int:\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "@node(output_name=\"unique_terms\")\n",
    "def count_unique_terms(tokens: List[str]) -> int:\n",
    "    return len(set(tokens))\n",
    "\n",
    "\n",
    "# Build heavy pipeline\n",
    "heavy_pipeline_hn = Pipeline(\n",
    "    nodes=[\n",
    "        clean_doc_text,\n",
    "        tokenize_doc,\n",
    "        compute_term_freq,\n",
    "        encode_doc_heavy,\n",
    "        compute_doc_length,\n",
    "        count_unique_terms,\n",
    "    ],\n",
    "    name=\"heavy_document_pipeline_hn\",\n",
    ")\n",
    "\n",
    "# Extract texts\n",
    "texts_heavy = [doc[\"text\"] for doc in synthetic_docs]\n",
    "\n",
    "# Create encoder\n",
    "encoder_heavy_hn = SimpleEncoder(dim=128, seed=42)\n",
    "\n",
    "print(\"Running HyperNodes heavy pipeline...\")\n",
    "start_hn_heavy = time.time()\n",
    "results_heavy_hn = heavy_pipeline_hn.map(\n",
    "    inputs={\"text\": texts_heavy, \"encoder\": encoder_heavy_hn}, map_over=\"text\"\n",
    ")\n",
    "elapsed_hn_heavy = time.time() - start_hn_heavy\n",
    "\n",
    "print(f\"\\nâ±ï¸  HyperNodes Heavy Pipeline:\")\n",
    "print(f\"   Processed: {len(results_heavy_hn['embedding'])} documents\")\n",
    "print(f\"   Time: {elapsed_hn_heavy:.4f}s\")\n",
    "print(f\"   Throughput: {len(texts_heavy) / elapsed_hn_heavy:.2f} docs/sec\")\n",
    "print(f\"   Avg doc length: {np.mean(results_heavy_hn['doc_length']):.1f} tokens\")\n",
    "print(f\"   Avg unique terms: {np.mean(results_heavy_hn['unique_terms']):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd18dd",
   "metadata": {},
   "source": [
    "### Daft Version - Without UDFs?\n",
    "\n",
    "**Note**: This heavy pipeline includes custom encoding logic (`hash % 100`) that has no built-in equivalent. While text cleaning could use built-ins (`.str.strip()`, `.str.lower()`, `.str.split()`), the encoding step requires a UDF.\n",
    "\n",
    "**Performance insight**: For large-scale data processing with custom transformations, UDFs are necessary but Daft's parallel execution makes them efficient. The 1.82x speedup over HyperNodes demonstrates Daft's advantage for bulk operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4112678e",
   "metadata": {},
   "source": [
    "### Daft Version - Heavy Pipeline with Batch Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6256c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee018b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c362a886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Daft heavy pipeline...\n",
      "  [Daft] Initializing HeavyEncoder dim=128\n",
      "\n",
      "â±ï¸  Daft Heavy Pipeline:\n",
      "   Processed: 5000 documents\n",
      "   Time: 0.9313s\n",
      "   Throughput: 5368.76 docs/sec\n",
      "   Avg doc length: 29.5 tokens\n",
      "   Avg unique terms: 14.8\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Performance Comparison:\n",
      "============================================================\n",
      "HyperNodes: 2.0589s (2428.46 docs/sec)\n",
      "Daft:       0.9313s (5368.76 docs/sec)\n",
      "Speedup:    2.21x\n",
      "============================================================\n",
      "\n",
      "â±ï¸  Daft Heavy Pipeline:\n",
      "   Processed: 5000 documents\n",
      "   Time: 0.9313s\n",
      "   Throughput: 5368.76 docs/sec\n",
      "   Avg doc length: 29.5 tokens\n",
      "   Avg unique terms: 14.8\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Performance Comparison:\n",
      "============================================================\n",
      "HyperNodes: 2.0589s (2428.46 docs/sec)\n",
      "Daft:       0.9313s (5368.76 docs/sec)\n",
      "Speedup:    2.21x\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Define Daft UDFs\n",
    "@daft.func\n",
    "def clean_doc_daft(text: str) -> str:\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "@daft.func\n",
    "def tokenize_doc_daft(text: str) -> list[str]:\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "@daft.func\n",
    "def compute_term_freq_daft(tokens: list[str]) -> dict:\n",
    "    \"\"\"Compute term frequency.\"\"\"\n",
    "    freq = {}\n",
    "    for token in tokens:\n",
    "        freq[token] = freq.get(token, 0) + 1\n",
    "    return freq\n",
    "\n",
    "\n",
    "@daft.func\n",
    "def compute_doc_length_daft(tokens: list[str]) -> int:\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "@daft.func\n",
    "def count_unique_terms_daft(tokens: list[str]) -> int:\n",
    "    return len(set(tokens))\n",
    "\n",
    "\n",
    "# Heavy encoder with batch support\n",
    "@daft.cls\n",
    "class HeavyEncoderDaft:\n",
    "    def __init__(self, dim: int, seed: int = 42):\n",
    "        print(f\"  [Daft] Initializing HeavyEncoder dim={dim}\")\n",
    "        time.sleep(0.5)\n",
    "        self.dim = dim\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    @daft.method.batch(return_dtype=DataType.python())\n",
    "    def encode_batch(self, texts: Series) -> Series:\n",
    "        \"\"\"Batch encode with vectorized operations.\"\"\"\n",
    "        # Generate embeddings for all texts at once\n",
    "        n = len(texts)\n",
    "        embeddings = self.rng.random((n, self.dim), dtype=np.float32)\n",
    "\n",
    "        # Vectorized normalization\n",
    "        norms = np.sqrt(np.sum(embeddings**2, axis=1, keepdims=True) + 1e-8)\n",
    "        embeddings = embeddings * norms\n",
    "\n",
    "        return Series.from_pylist(list(embeddings))\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df_heavy = daft.from_pydict(\n",
    "    {\n",
    "        \"doc_id\": [d[\"doc_id\"] for d in synthetic_docs],\n",
    "        \"text\": [d[\"text\"] for d in synthetic_docs],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create encoder\n",
    "encoder_heavy_daft = HeavyEncoderDaft(dim=128, seed=42)\n",
    "\n",
    "print(\"\\nRunning Daft heavy pipeline...\")\n",
    "start_daft_heavy = time.time()\n",
    "\n",
    "# Build pipeline\n",
    "df_heavy = df_heavy.with_column(\"cleaned\", clean_doc_daft(df_heavy[\"text\"]))\n",
    "df_heavy = df_heavy.with_column(\"tokens\", tokenize_doc_daft(df_heavy[\"cleaned\"]))\n",
    "df_heavy = df_heavy.with_column(\"term_freq\", compute_term_freq_daft(df_heavy[\"tokens\"]))\n",
    "df_heavy = df_heavy.with_column(\n",
    "    \"embedding\", encoder_heavy_daft.encode_batch(df_heavy[\"cleaned\"])\n",
    ")\n",
    "df_heavy = df_heavy.with_column(\n",
    "    \"doc_length\", compute_doc_length_daft(df_heavy[\"tokens\"])\n",
    ")\n",
    "df_heavy = df_heavy.with_column(\n",
    "    \"unique_terms\", count_unique_terms_daft(df_heavy[\"tokens\"])\n",
    ")\n",
    "\n",
    "# Materialize\n",
    "results_heavy_daft = df_heavy.collect()\n",
    "elapsed_daft_heavy = time.time() - start_daft_heavy\n",
    "\n",
    "# Get results as dict\n",
    "heavy_dict = results_heavy_daft.to_pydict()\n",
    "\n",
    "print(f\"\\nâ±ï¸  Daft Heavy Pipeline:\")\n",
    "print(f\"   Processed: {results_heavy_daft.count_rows()} documents\")\n",
    "print(f\"   Time: {elapsed_daft_heavy:.4f}s\")\n",
    "print(f\"   Throughput: {len(synthetic_docs) / elapsed_daft_heavy:.2f} docs/sec\")\n",
    "print(f\"   Avg doc length: {np.mean(heavy_dict['doc_length']):.1f} tokens\")\n",
    "print(f\"   Avg unique terms: {np.mean(heavy_dict['unique_terms']):.1f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"ðŸ“Š Performance Comparison:\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(\n",
    "    f\"HyperNodes: {elapsed_hn_heavy:.4f}s ({len(texts_heavy) / elapsed_hn_heavy:.2f} docs/sec)\"\n",
    ")\n",
    "print(\n",
    "    f\"Daft:       {elapsed_daft_heavy:.4f}s ({len(synthetic_docs) / elapsed_daft_heavy:.2f} docs/sec)\"\n",
    ")\n",
    "print(f\"Speedup:    {elapsed_hn_heavy / elapsed_daft_heavy:.2f}x\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd83ed9f",
   "metadata": {},
   "source": [
    "### Key Observations - Heavy Pipeline\n",
    "\n",
    "**What we're testing:**\n",
    "- Processing 5000 synthetic documents\n",
    "- Each document: 10-50 words from a 20-word vocabulary\n",
    "- Multi-stage pipeline: clean â†’ tokenize â†’ compute stats â†’ encode (128D) â†’ aggregate\n",
    "\n",
    "**Performance factors:**\n",
    "\n",
    "1. **HyperNodes strengths**:\n",
    "   - Very low overhead for simple operations\n",
    "   - Efficient for CPU-bound row-wise processing\n",
    "   - No serialization overhead for in-process execution\n",
    "   - Direct Python execution\n",
    "\n",
    "2. **Daft strengths** (become more apparent with):\n",
    "   - Larger datasets (10K+ documents)\n",
    "   - More complex aggregations\n",
    "   - Operations that benefit from columnar processing\n",
    "   - Distributed execution needs\n",
    "   - When using batch UDFs with vectorized operations\n",
    "\n",
    "3. **The real advantage**: Daft's batch processing can be **significantly faster** when:\n",
    "   - You leverage `.batch()` methods with NumPy/PyArrow\n",
    "   - Data doesn't fit in memory (streaming)\n",
    "   - You need distributed processing\n",
    "   - Operations are vectorizable (matrix operations, aggregations)\n",
    "\n",
    "**Try this**: Change `num_docs` to 50,000 or add more complex numpy operations in the encoder to see Daft's advantages grow!\n",
    "\n",
    "**Takeaway**: For small-to-medium datasets with simple operations, HyperNodes' simplicity wins. For large-scale, vectorizable workloads, Daft's optimization and batch processing shine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea65af05",
   "metadata": {},
   "source": [
    "## Summary: Translation Patterns\n",
    "\n",
    "### 1. Simple Transformations\n",
    "**HyperNodes:**\n",
    "```python\n",
    "@node(output_name=\"result\")\n",
    "def transform(x: int) -> int:\n",
    "    return x * 2\n",
    "```\n",
    "\n",
    "**Daft:**\n",
    "```python\n",
    "@daft.func\n",
    "def transform(x: int) -> int:\n",
    "    return x * 2\n",
    "\n",
    "df = df.with_column(\"result\", transform(df[\"x\"]))\n",
    "```\n",
    "\n",
    "### 2. Map Operations\n",
    "**HyperNodes:**\n",
    "```python\n",
    "pipeline.map(inputs={\"items\": data}, map_over=\"items\")\n",
    "```\n",
    "\n",
    "**Daft:**\n",
    "```python\n",
    "df = daft.from_pydict({\"items\": data})\n",
    "df = df.with_column(\"result\", func(df[\"items\"]))\n",
    "```\n",
    "\n",
    "### 3. Stateful Processing\n",
    "**HyperNodes:**\n",
    "```python\n",
    "encoder = Encoder()  # Initialize once\n",
    "pipeline.map(inputs={\"text\": texts, \"encoder\": encoder}, map_over=\"text\")\n",
    "```\n",
    "\n",
    "**Daft:**\n",
    "```python\n",
    "@daft.cls\n",
    "class Encoder:\n",
    "    def __init__(self): ...\n",
    "    @daft.method(...)\n",
    "    def encode(self, text): ...\n",
    "\n",
    "encoder = Encoder()  # Lazy init\n",
    "df = df.with_column(\"encoded\", encoder.encode(df[\"text\"]))\n",
    "```\n",
    "\n",
    "### 4. Batch Operations\n",
    "**HyperNodes:**\n",
    "```python\n",
    "# Manual batching or row-wise processing\n",
    "```\n",
    "\n",
    "**Daft:**\n",
    "```python\n",
    "@daft.func.batch(return_dtype=...)\n",
    "def process_batch(series: Series) -> Series:\n",
    "    # Vectorized operations\n",
    "    return result_series\n",
    "```\n",
    "\n",
    "### Key Advantages of Daft\n",
    "\n",
    "1. **Lazy Evaluation**: Daft optimizes the entire pipeline before execution\n",
    "2. **Automatic Parallelization**: No need to manually configure parallelism\n",
    "3. **Batch Processing**: Easy to leverage vectorized operations\n",
    "4. **Generator Support**: Built-in support for one-to-many transformations\n",
    "5. **Struct Unnesting**: Elegant handling of nested data structures\n",
    "6. **Scalability**: Designed for distributed execution\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "**Use HyperNodes when:**\n",
    "- You need explicit DAG visualization and control\n",
    "- You want fine-grained caching at the node level\n",
    "- Your pipeline has complex branching logic\n",
    "- You need to inspect intermediate results easily\n",
    "\n",
    "**Use Daft when:**\n",
    "- Performance is critical\n",
    "- You're processing large datasets\n",
    "- You want automatic optimization\n",
    "- You need distributed execution\n",
    "- Your operations can be vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c307c",
   "metadata": {},
   "source": [
    "## When to Use Built-in Operations vs UDFs\n",
    "\n",
    "### âœ… Prefer Built-in Operations When:\n",
    "\n",
    "1. **String Operations** (limited): `.str.contains()`, `.str.split()`\n",
    "   - âš ï¸ **Note**: Common operations like `.str.strip()`, `.str.lower()`, `.str.upper()`, `.str.replace()` are NOT available as built-ins\n",
    "2. **List Operations**: `.list.length()`, `.list.get()`, `.explode()`, `.list.join()`\n",
    "3. **Arithmetic**: `+`, `-`, `*`, `/`, `%` work on columns directly\n",
    "4. **Struct Access**: `df[\"struct\"][\"field\"]` for nested field access\n",
    "5. **Aggregations**: `.sum()`, `.mean()`, `.count()`, etc.\n",
    "\n",
    "**Why?** Built-ins are optimized, well-tested, and often faster than custom UDFs when available.\n",
    "\n",
    "### âš ï¸ Use UDFs When:\n",
    "\n",
    "1. **Text Cleaning**: `.strip()`, `.lower()`, `.upper()` etc. require UDFs (not available as built-ins)\n",
    "2. **Custom Logic**: Business logic that doesn't map to built-ins (e.g., custom encoding, validation)\n",
    "3. **Expensive Initialization**: Loading models, connecting to databases (`@daft.cls` for stateful processing)\n",
    "4. **Complex Transformations**: Multi-step logic that's clearer as a function\n",
    "5. **External Libraries**: Calling specialized libraries (e.g., ML models, scientific computing)\n",
    "6. **Batch Processing**: Vectorized operations on NumPy arrays (`@daft.func.batch`)\n",
    "\n",
    "**Why?** Some operations simply can't be expressed with built-ins, and UDFs provide the flexibility needed.\n",
    "\n",
    "### ðŸŽ¯ Best Practice: Mix and Match\n",
    "\n",
    "Real-world pipelines typically combine both:\n",
    "- Use built-ins for standard transformations (string splitting, list operations, struct access)\n",
    "- Use UDFs for text cleaning and custom business logic\n",
    "- Profile to identify bottlenecks\n",
    "\n",
    "### Performance Hierarchy (fastest to slowest)\n",
    "\n",
    "1. **Built-in operations** - Optimized Rust/C++ implementations\n",
    "2. **Batch UDFs** (`@daft.func.batch`) - Process multiple rows at once\n",
    "3. **Class UDFs** (`@daft.cls`) - Stateful with initialization overhead\n",
    "4. **Simple UDFs** (`@daft.func`) - Per-row Python function calls\n",
    "\n",
    "Choose the simplest tool that solves your problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2819083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Tutorial Complete!\n",
      "============================================================\n",
      "\n",
      "Key Takeaways:\n",
      "1. Daft uses DataFrame operations instead of explicit pipelines\n",
      "2. @daft.func for simple transformations\n",
      "3. @daft.cls for stateful operations with initialization\n",
      "4. @daft.func.batch for high-performance vectorized operations\n",
      "5. Generators and struct unnesting provide elegant data shaping\n",
      "6. Lazy evaluation enables automatic optimization\n",
      "\n",
      "Both frameworks have their place - choose based on your needs!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Tutorial Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Daft uses DataFrame operations instead of explicit pipelines\")\n",
    "print(\"2. @daft.func for simple transformations\")\n",
    "print(\"3. @daft.cls for stateful operations with initialization\")\n",
    "print(\"4. @daft.func.batch for high-performance vectorized operations\")\n",
    "print(\"5. Generators and struct unnesting provide elegant data shaping\")\n",
    "print(\"6. Lazy evaluation enables automatic optimization\")\n",
    "print(\"\\nBoth frameworks have their place - choose based on your needs!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypernodes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

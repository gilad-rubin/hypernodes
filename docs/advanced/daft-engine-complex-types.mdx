---
title: Complex Types in Daft
description: 'Handling Pydantic models and nested structures in DaftEngine.'
---

DaftEngine seamlessly handles complex Python types (Pydantic models, Lists, Dicts) by using Daft's Python object storage.

## Overview

Daft's native type system is optimized for primitives (int, float, string). However, HyperNodes pipelines often use complex objects. DaftEngine bridges this gap by automatically detecting complex types and storing them as Python objects (`DataType.python()`).

## Supported Types

- **Pydantic Models**: Preserves full model validation and methods
- **Nested Structures**: `List[List[int]]`, `Dict[str, Any]`
- **Custom Classes**: Arbitrary user-defined classes
- **Mixed Types**: Lists containing different types

## Examples

### List of Pydantic Models

```python
from pydantic import BaseModel
from typing import List
from hypernodes import node, Pipeline, DaftEngine

class Document(BaseModel):
    id: str
    text: str

@node(output_name="documents")
def create_documents(count: int) -> List[Document]:
    return [Document(id=f"doc_{i}", text=f"Text {i}") for i in range(count)]

pipeline = Pipeline(nodes=[create_documents], engine=DaftEngine())
result = pipeline.run(inputs={"count": 3})
# result["documents"] is a list of Document objects
```

### Nested Dictionaries

```python
from typing import Dict, Any

@node(output_name="config")
def create_config(name: str) -> Dict[str, Any]:
    return {"name": name, "nested": {"key": "value"}}

pipeline = Pipeline(nodes=[create_config], engine=DaftEngine())
result = pipeline.run(inputs={"name": "test"})
```

## Performance vs Flexibility

### Python Object Storage (Default)

**Pros:**
- Works with **any** Python type
- Zero boilerplate (no schema definition)
- Preserves object methods and identity

**Cons:**
- Slower than native Daft types (serialization overhead)
- Cannot be pushed down to Arrow/Parquet layers efficiently

### Native Structs (Optimization)

For maximum performance with Pydantic, you can manually convert to Daft Structs:

```python
# Advanced: Manual optimization
import pyarrow as pa

@daft.udf(return_dtype=daft.DataType.struct({"id": daft.DataType.string(), ...}))
def optimized_func(x):
    ...
```

**Recommendation**: Start with the default Python object storage. Only optimize to native structs if profiling shows serialization is a bottleneck.

## See Also

- [Automatic Type Inference](./automatic-type-inference-for-daft)
- [DaftEngine Overview](./daft-engine)

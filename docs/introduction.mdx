---
title: Introduction
description: 'Build once, cache intelligently, run anywhere.'
---

<img
  className="block dark:hidden"
  src="/assets/light_background_logo.png"
  alt="Hero Light"
/>
<img
  className="hidden dark:block"
  src="/assets/dark_background_logo.png"
  alt="Hero Dark"
/>

**HyperNodes** is a hierarchical, modular pipeline system designed for ML/AI workflows. It allows you to build complex workflows from simple, reusable functions and scale them from your laptop to a distributed cluster without changing a single line of code.

## The 30-Second Hook

Write standard Python functions. Decorate them. Run them.

```python
from hypernodes import node, Pipeline

@node
def load_data(path: str) -> str:
    print(f"Loading {path}...")
    return "raw data"

@node
def process(data: str) -> str:
    return data.upper()

# Create the pipeline (HyperNodes builds the graph automatically)
pipe = Pipeline(nodes=[load_data, process])

# Run it
result = pipe.run(inputs={"path": "file.txt"})
print(result)
# Output: {'process': 'RAW DATA'}
```

## Why HyperNodes?

<CardGroup cols={2}>
  <Card title="Test with One, Scale to Many" icon="flask">
    Build and test your pipeline with a single input. When you're ready, run it over thousands of inputs. HyperNodes handles the batching and parallelism for you.
    ```python
    results = pipe.map(inputs={"path": ["file1.txt", ...]}, map_over="path")
    ```
  </Card>
  <Card title="Run Anywhere" icon="server">
    Swap the **Engine** to run on a cluster.
    ```python
    # Now it runs distributed!
    pipe = Pipeline(nodes=[...], engine=DaftEngine())
    ```
  </Card>
  <Card title="Intelligent Caching" icon="database">
    Don't re-run what hasn't changed. HyperNodes automatically caches results based on function code, inputs, and upstream dependencies.
  </Card>
  <Card title="Hierarchical Modularity" icon="sitemap">
    Functions are nodes. Pipelines are nodes. Nest pipelines infinitely to compose complex systems from simple, testable blocks.
  </Card>
</CardGroup>

## Installation

```bash
pip install hypernodes
# or with uv
uv add hypernodes
```

To use the distributed engine features:

```bash
pip install "hypernodes[daft]"
```

## Documentation Guide

<CardGroup cols={2}>
  <Card title="Core Concepts" icon="book" href="/docs/essentials/nodes">
    Learn about Nodes, Pipelines, and Execution.
  </Card>
  <Card title="Data Processing" icon="layer-group" href="/docs/data-processing/mapping">
    Master Mapping, Nesting, and Caching.
  </Card>
  <Card title="Scaling" icon="rocket" href="/docs/scaling/engines">
    Scale to big data with Daft and Stateful parameters.
  </Card>
  <Card title="Observability" icon="chart-line" href="/docs/observability/visualization">
    Visualize and monitor your pipelines.
  </Card>
</CardGroup>

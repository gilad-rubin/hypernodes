---
title: Mapping
description: 'Parallel processing over lists.'
---

One of the core design principles of HyperNodes is **"Test with One, Scale to Many"**.

You design your pipeline to handle a single item (a single document, image, or row of data). When it comes time to process a dataset, you don't write a `for` loopâ€”you use `.map()`.

## The `map()` Method

The `map()` method executes the pipeline over a collection of inputs.

```python
@node
def square(x: int) -> int:
    return x * x

pipeline = Pipeline(nodes=[square])

# Input is a LIST of values
results = pipeline.map(
    inputs={"x": [1, 2, 3, 4]}, 
    map_over="x"
)
# results = [{'square': 1}, {'square': 4}, ...]
```

### `map_over`

The `map_over` argument tells HyperNodes which input parameters should be iterated over. 

- **Single Argument**: `map_over="x"` iterates over the list provided for `x`.
- **Multiple Arguments**: `map_over=["x", "y"]` iterates over both `x` and `y`.

## Mapping Modes

When mapping over multiple arguments, you can choose how they are combined.

### Zip Mode (Default)

`map_mode="zip"` iterates over inputs in parallel, like Python's `zip()`.

```python
# x=[1, 2], y=[10, 20]
# Pairs: (1, 10), (2, 20)
results = pipeline.map(
    inputs={"x": [1, 2], "y": [10, 20]},
    map_over=["x", "y"],
    map_mode="zip"
)
```

### Product Mode (Grid Search)

`map_mode="product"` creates a Cartesian product of all inputs. This is extremely useful for hyperparameter tuning or grid searches.

```python
@node(output_name="score")
def evaluate_param(param_a: int, param_b: int) -> int:
    return param_a * param_b

grid_pipeline = Pipeline(nodes=[evaluate_param])

# Executes 2 * 2 = 4 combinations
grid_results = grid_pipeline.map(
    inputs={
        "param_a": [1, 2], 
        "param_b": [10, 20]
    },
    map_over=["param_a", "param_b"],
    map_mode="product",
)
# Results: 
# (1, 10) -> 10
# (1, 20) -> 20
# (2, 10) -> 20
# (2, 20) -> 40
```

## Fixed Inputs

Arguments *not* included in `map_over` are treated as constants and passed to every execution.

```python
@node
def add_scalar(vector: int, scalar: int) -> int:
    return vector + scalar

pipeline = Pipeline(nodes=[add_scalar])

results = pipeline.map(
    inputs={
        "vector": [1, 2, 3],  # Iterated
        "scalar": 10          # Constant
    },
    map_over="vector"
)
# Results: 11, 12, 13
```

## Why not just use a loop?

1.  **Parallelism**: If you use a parallel engine (like `DaskEngine` or `DaftEngine`), `.map()` automatically distributes the work.
2.  **Caching**: HyperNodes caches each item individually. If you add 10 items to a list of 1000, only the 10 new items are processed.
3.  **Batching**: Engines like `DaftEngine` can automatically batch these operations for vectorization.


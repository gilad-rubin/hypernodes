---
title: Nesting
description: 'Composing pipelines.'
---

HyperNodes treats Pipelines as Nodes. This allows for **Hierarchical Modularity**, enabling you to build complex systems from simple, reusable components.

## `as_node()`

To use a Pipeline inside another Pipeline, you convert it into a node using `.as_node()`.

```python
# 1. Define a sub-pipeline
inner = Pipeline(nodes=[clean, tokenize])

# 2. Convert to node
inner_node = inner.as_node(name="text_processor")

# 3. Use in outer pipeline
outer = Pipeline(nodes=[load_data, inner_node, save_data])
```

## Interface Mapping

Often, the inner pipeline's variable names don't match the outer pipeline's. `.as_node()` allows you to map them.

Consider a text cleaning pipeline that expects `text` and produces `cleaned`:

```python
@node
def clean(text: str) -> str: ...
```

And an outer pipeline that has `document` and needs `processed_doc`:

```python
processor = inner.as_node(
    input_mapping={"document": "text"},        # Outer "document" -> Inner "text"
    output_mapping={"cleaned": "processed_doc"} # Inner "cleaned" -> Outer "processed_doc"
)
```

## Internal Mapping (The Fractal Pattern)

A powerful feature of nesting is the ability to run a loop *inside* a node. 

Imagine `inner` processes a **single** document. You can configure it to process a **list** of documents when used in the outer pipeline.

```python
# Inner: Takes str -> returns str
# Outer: Takes List[str] -> returns List[str]

batch_processor = inner.as_node(
    map_over="text"  # Tell inner pipeline to map over "text" input
)
```

From the perspective of the outer pipeline, `batch_processor` is just a node that takes a list and returns a list. Internally, it's running a pipeline loop.

## Configuration Inheritance

Nested pipelines inherit the execution engine of their parent by default, but can override it. This allows you to run specific parts of your graph on different hardware (e.g., a GPU-intensive sub-pipeline) if the Engine supports it.

